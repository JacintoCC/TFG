\chapter*{}
%\thispagestyle{empty}
%\cleardoublepage

%\thispagestyle{empty}

\input{portada/portada_2}


\cleardoublepage
\thispagestyle{empty}

\begin{center}
{\large\bfseries \myTitle }\\
\end{center}
\begin{center}	
	\myName \\
\end{center}


\vspace{0.7cm}
\noindent{\textbf{Resumen}}\\

	En este trabajo se pretende realizar un estudio sobre
	la aplicación de test estadísticos en la comparación
	del rendimiento de algoritmos de aprendizaje automático.
	Se incluye la descripción matemática de los test más
	utilizados tanto a la hora de detectar diferencias 
	entre los algoritmos como para evaluar el cumplimiento
	de ciertas características de los datos para la
	realización de los test. Se presentan test de 
	hipótesis nula basados en la estadística frecuentista,
	tanto paramétricos como no paramétricos, y test
	bayesianos. Para mostrar un ejemplo del problema, se 
	muestra la aplicación de test estadísticos sobre
	los resultados de varios algoritmos de clasificación
	sobre diversos conjuntos de datos y se buscan las 
	diferencias en el rendimiento de estos algoritmos.
	Este estudio nos permitirá observar las propiedades
	de cada test y realizar la comparación entre la
	potencia de cada uno de ellos.
	
%\vspace{0.7cm}
\noindent{\textbf{Palabras clave}: aprendizaje automático,
	evaluación de algoritmos, test paramétricos, test no 
	paramétricos, test basados en permutaciones, test 
	bayesianos, R}\\
\cleardoublepage

\thispagestyle{empty}


\begin{center}
{\large\bfseries Evaluation of state of the art stadistic techniques for machine learning algorithms' comparative analysis}\\
\end{center}
\begin{center}
	\myName	\\
\end{center}

\vspace{0.7cm}
\noindent{\textbf{Abstract}}\\

	In recent years the interest in machine learning has 
meant an increasing amount in the proposed machine learning 
algorithms. Nevertheless, these new algorithms need to be 
tested and the properties and results claimed by the 
algorithms’ authors must be proved, so in order to achieve 
this objective statistical tests are the best and the most 
common approach.\\
	In this project, we present a set of tests commonly used 
in this problem, a computer tool that gathers different 
packages to make all tests described and a practical case of 
study analysing the power of each test. \\
	In the first part of this memory, we present the 
mathematical base of the statistical tests. This part 
contains an introduction about measuring the algorithms’ 
performance and the ways to obtain a reliable measure of this 
performance. The tests presented are classified in three 
categories: parametric, non-parametric and Bayesian tests. In 
each categorie we include tests with different goals: 
comparison between two algorithms in one data set, two 
algorithms in multiple data sets or between multimple 
algoritms in multiple data sets. The first set of statistical 
tests presented are the parametric ones, which are widely 
known and used, like t-test or ANOVA, but presents some 
faults related with the necessary conditions to apply the 
parametric tests, which are rarely given. \\
	We describe these faults and present the non parametric 
tests, which do not need these strict conditions about the 
population like normality or homocedasticity but relaxed 
conditions like the symmetry or the continuity of the 
population. Non parametric test are usually less powerful 
than parametric one due to in non parametric test we have 
less information about the underlying population of the data, 
but this lack of power is compensated with the lack of 
precision in the parametric tests when the number of data is 
low or the conditions are not satisfied. The tests presented 
in the non parametric chapter allows us to obtain some 
valuable information as the randomness of a sample (with  the 
number of runs tests) or the goodness of fit of the data to a 
given population (with the chi-square test, Kolmogorov-
Smirnov test...), which are conditions to use the previous 
parametric tests. Moreover, non parametric tests as Wilcoxon 
signed-ranks test for comparing two algorithms or Friedman 
test for multiple algorithms over multiple data sets are 
presented in this chapter with some improvements like Iman-
Davenport test or Quade test, which takes account of the 
different difficulties of the data sets. Once the null 
hypothesis of an equal performance of all algorithms in the 
study is rejected, we need a post-hoc method to find where is 
the difference between algorithms and adjusting the $p$-
value, so we introduce some procedures, like Finner’s, 
Hommel’s or Li’s procedures. As a special case of non 
parametric tests, we include a general description of the 
base of permutation tests, which start with a sample of two 
populations and collect information against the null 
hypothesis making permutations of the sample. \\
	As a complete different approach of the comparison, we 
present Bayesian tests. Null hypothesis tests present some 
problems mainly related with the understanding of what 
information they give. More frequently that it should 
happens, researchers thinks that the $p$-value given by a 
null hypothesis test corresponds to the probability, given a 
sample, that the null hypothesis is true instead of the 
actual meaning, that is the probability, assuming the null 
hypothesis is true, of obtaining a sample as extreme or more 
than our sample. The wanted statement is not achieved with 
frequentist statistical methods but Bayesian ones, so we 
present a series of differences between null hypothesis tests 
and Bayesians tests. Other desirable property is that 
decision based on Bayesian tests is not dichotomic as in the 
frequentist ones, but we obtain a distribution of the 
parameters of the underlying population. The tests presented 
are a Bayesian version of frequentist tests as t-test, sign 
test and Wilcoxon signed-rank test.\\
	We dedicate the second part of the memory to the 
experimental study of the comparison between algorithms. 
The first chapter contains a revision of state of the art 
statistical techniques applied in recent years as well as 
most influential articles in the field of machine learning.  
Ditterich and  Alpaydin were the first authors that proposed 
a specific method to make comparisons between algorithms in a 
statistical way and had a great influence on posterior 
experiments. Among non parametric tests articles, the 
proposal of Dem\v{s}ar and its extension made by García 
\textit{et. al} set up the basis for this kind of 
comparisons. More recently, authors like Benavoli and Corani 
has proposed Bayesian methods to compare the performance of 
machine learning algorithms. \\
	The computer tool developed consists in a wrapper for 
\texttt{R} of different packages of statistical analysis. The 
main goal of the developed package is using in \texttt{R} the 
\texttt{Java} package of non parametric tests developed by 
Derrac \textit{et. al} and it is possible thanks to 
\texttt{rJava}. Others methods included in the packages come 
from the \texttt{scmamp} package, which is dedicated to the 
comparison between multiple algorithms over multiple data 
sets. Finally, we make an implementation of the Bayesian test 
previously described based on the work made by Benavoli. \\
	Finally, we present a case of study to illustrate the 
concepts used in this document, comparing five classical 
machine learning algorithms for classification problems 
(logistic regression, $k$-NN, Naive Bayes classifier, neural 
networks and random forest) over multiple data sets and 
applying the methods implemented. We include an study of the 
power of each test. 

%\vspace{0.7cm}
\noindent{\textbf{Keywords}: Machine learning, algorithms' 
evaluation, parametric tests, non parametric tests,
permutation tests, bayesian tests, R}\\



\chapter*{}
\thispagestyle{empty}

\noindent\rule[-1ex]{\textwidth}{2pt}\\[4.5ex]

Yo, \textbf{\myName}, alumno de la titulación \myDegree de la \textbf{\myFaculty de la \myUni}, con DNI 32056356-Z, autorizo la
ubicación de la siguiente copia de mi Trabajo Fin de Grado en la biblioteca de ambos centros para que pueda ser
consultada por las personas que lo deseen.

\vspace{6cm}

\noindent Fdo: \myName

\vspace{2cm}

\begin{flushright}
Granada a 9 de septiembre de 2016.
\end{flushright}


\chapter*{}
\thispagestyle{empty}

\noindent\rule[-1ex]{\textwidth}{2pt}\\[4.5ex]

D. \textbf{ \myProf }, Profesor del Departamento de Ciencias de la Computación e Inteligencia Artificial de la Universidad de Granada.

\vspace{0.5cm}

D. \textbf{ \myOtherProf}, Profesora del Departamento de Estadística e Investigación Operativa de la Universidad de Granada.


\vspace{0.5cm}

\textbf{Informan:}

\vspace{0.5cm}

Que el presente trabajo, titulado \textit{\textbf{\myTitle}},
ha sido realizado bajo su supervisión por \textbf{\myName}, y autorizamos la defensa de dicho trabajo ante el tribunal
que corresponda.

\vspace{0.5cm}

Y para que conste, expiden y firman el presente informe en Granada a 9 de septiembre de 2016.

\vspace{1cm}

\textbf{Los directores:}

\vspace{5cm}

\noindent \textbf{\myProf \ \ \ \ \ \ \myOtherProf}

\chapter*{Agradecimientos}
\thispagestyle{empty}

       \vspace{1cm}


Poner aquí agradecimientos...

