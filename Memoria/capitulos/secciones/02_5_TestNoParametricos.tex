 
 	Se incluye en esta sección el desarrollo de los test no paramétricos más utilizados en aprendizaje automático.
 	
 	
\subsection{Test de aleatoriedad}

	Una de las condiciones para la realización de los test estadísticos, tanto de los paramétricos como de los no paramétricos, es la aleatoriedad de la muestra de partida. La hipótesis nula para los test que serán presentados en esta sección será la aleatoriedad de la muestra, mientras que la hipótesis alternativa será la presencia de un patrón. Estos test también son útiles en los estudios de series temporales y control de calidad.
	
\subsubsection{Test basado en el número de rachas}

	Supongamos una secuencia de $n$ elementos de dos tipos, $n_1$ del primer tipo y $n_2$ del segundo, $n = n_1 + n_2$. Sea $R_1$ el número de rachas del primer tipo, $R_2$ el número de rachas del segundo tipo, $R_ = R_1 + R_2$. Siendo cierta la hipótesis nula (la aleatoriedad de la muestra), procedemos a obtener la distribución de $R$.
	
\begin{lema} 
	El número de formas distintas de distribuir $n$ objetos en $r$ posiciones consecutivas es ${n-1 \choose r-1}, n \geq r, r \geq 1$.
\end{lema}

\begin{teorema}
	Sean $R_1$ y $R_2$ los números de rachas de los $n_1$ de tipo 1 y los $n_2$ elementos de tipo 2 respectivamente en una muestra de tamaño $n = n_1 + n_2$. La función de distribución de probabilidad conjunta de $R_1$ y $R_2$ es
	\[ f_{R_1,R_2} (r_1, r_2) = 
		\frac{c {n_1 - 1 \choose r_1 - 1} 
				{n_2 - 1 \choose r_2 - 1}}
			{{n_1 + n_2 \choose n_1}}\;
		\begin{array}{l}
			r_1 = 1,2, \dots, n_1 \\
			r_2 = 1,2, \dots, n_2 \\
			r_1 = r_2 \textit{ or } r_1 = r_2 \pm 1
		\end{array}
	\]
	donde $c=2$ si $r_1 = r_2$ (hay igual número de rachas de elementos del tipo 1 y del tipo 2) y $c=1$ si $r_1 = r_2 \pm 1$ (hay una racha más del tipo 1 ó 2).
\end{teorema}

	Para muestras de un mayor tamaño (aquellas en el que $n_1, n_2 \geq 10$) se suele utilizar una aproximación utilizando la distribución asintótica supuesto cierta $H_0$.\\
	Suponemos que el tamaño de la muestra $n \rightarrow \infty$, de forma en que $\frac{n_1}{n} \rightarrow \lambda$, $0<\lambda<1$. De aquí obtenemos
	\[ \underset{\lim}{n \rightarrow \infty} E[R/n] = 
			2\lambda (1-\lambda) 
				\underset{\lim}{n \rightarrow \infty} 
					var(R\sqrt{n}) =
			4\lambda^2(1-\lambda)^2
	\]
	
\begin{teorema}
	La distribución de probabilidad de $R$, es decir, el número total de rachas en una muestra aleatoria es:
	
	\begin{equation}
		f_R(r) = \left\lbrace\begin{array}{ll}
	2 {n_1-1 \choose r/2-1} {n_2-1 \choose r/2-1} 
		\bigg/ {n_1 + n_2 \choose n_1} &
			\textit{ si } r \text{ es par} \\
	\left[
		{n_1-1 \choose (r-1)/2} {n_2-1 \choose (r-3)/2} +  
		{n_1-1 \choose (r-3)/2} {n_2-1 \choose (r-1)/2} 
	\right]
		\bigg/ {n_1 + n_2 \choose n_1} &
			\textit{ si } r \text{ es impar} \\		
		\end{array}\right.
	\label{th-dist-R}
	\end{equation}
	para $r=2, 3, \dots, n_1 + n_2.$
\end{teorema}
	
	Si llamamos $Z = \frac{R - 2n\lambda (1-\lambda)}{2 \sqrt{n}\lambda (1-\lambda)}$ y sustituimos en \ref{th-dist-R}, obtenemos la distribución estandarizada de $R$, $f_Z(z)$. Entonces aplicamos la fórmula de Stirling y el límite queda de la forma
	\[ \underset{\lim}{n \rightarrow \infty} \log f_Z(z)=
			-\log \sqrt{2\pi} - \frac{1}{2} z^2	\]
	con lo que la distribución límite de $Z$ es la normal. 
	
	
\subsubsection{Test basado en rachas crecientes y decrecientes}	

	Para este test consideramos una serie de datos de tipo numérico ordenados temporalmente y queremos comprobar la hipótesis de la aleatoriedad de la muestra.\\
	Para una muestra de $n$ elementos, supongamos que podemos ordenarlos de la forma $a_1 < \dots < a_n$ (estamos suponiendo que no hay dos iguales. Si la hipótesis nula fuese cierta, nuestra muestra se corresponderá con una de las $n!$ permutaciones con igual probabilidad. Usaremos para este test las rachas crecientes y decrecientes. Construimos la secuencia $D_{n-1}$, cuyo elemento $i$-ésimo es el signo de $x_{i+1} - x_i,\ i=1, \dots, n-1$. Sean $R_1, \dots, R_{n-1}$ el número de rachas de longitud $1, \dots, n-1$ respectivamente. $f_n(r_{n-1}, \dots, r_1)$ indica la probabilidad de obtener $r_j$ rachas de longitud $j$ supuesta cierta la hipótesis nula. Escribiremos como $u_n$ la frecuencia absoluta $f_n = \frac{u_n}{n!}$. Para obtener la función de distribución, partiremos del caso $n=3$\\
	Sean $a_1 < a_2 < a_3$. La distribución de probabilidad será 
	\[ 
	f_3(r_2, r_1) = 
		\left\lbrace\begin{array}{cc}
			\frac{2}{6} & \text{si } r_2 = 1, r_1 = 0 \\
			\frac{4}{6} & \text{si } r_2 = 0, r_1 = 2 
		\end{array}\right.
	\]
	dado que las únicas rachas de longitud 2 son $(a_1, a_2, a_3) \rightarrow (+,+)$ y $(a_3, a_2, a_1) \rightarrow (-,-)$, siendo las demás posibles combinaciones dos rachas de longitud 1.\\
	Las posibilidades a la hora de insertar un elemento $a_n$ en las permutaciones de $S_n$ son:
\begin{enumerate}
	\item Se añade una racha de longitud 1.
	\item Una racha de longitud $i-1$ se convierte en una de longitud $i$, $i=2,\dots n-1$.
	\item Una racha de longitud $h=2i$ se convierte en una de longitud $i$, seguida por otra de longitud $1$, seguida por otra de longitud $i$.
	\item Una racha de longitud $h=i+j$ se convierte en
	\begin{enumerate}
		\item Una racha de longitud $i$ seguida por otra de longitud $1$ seguida por otra de longitud $j$.
		\item Una racha de longitud $j$ seguida por otra de longitud $1$ seguida por otra de longitud $i$.
	\end{enumerate}
	con $h>i>j$, $3 \leq h \leq n-2$
\end{enumerate}

	De forma general, la frecuencia $u_n$ conocido $u_{n-1}$ sigue la siguiente relación:
\begin{align*}
	& u_n (r_{n-1}, \dots, r_h, \dots, r_i, \dots, r_j, \dots, r_1)= 
		2 u_{n-1}(r_{n-2}, \dots, r_1-1) \\
	&+ \sum\limits_{i=2}^{n-1} 
		(r_{i-1} + 1)
		u_{n-1}(r_{n-2},\dots, r_i-1, r_{i-1}+1,\dots, r_1)\\
	&+ \sum\limits_{i=1, h=2i}^{\lfloor (n-2)/2 \rfloor} 
		(r_{h} + 1)
		u_{n-1}(r_{n-2},\dots, r_h+1,\dots r_i-2,\dots, r_1-1)\\
	&+ 2 \underbrace{\sum\limits_{i=2}^{n-3} \sum\limits_{j=1}^{i-1}}_{h=i+j, h \leq n-2}
		(r_{h} + 1)
		u_{n-1}(r_{n-2},\dots, r_h+1,\dots, r_i-1,\dots, r_1-1)			
\end{align*}
	
	Otro test que se podría realizar es el del número total de rachas, independientemente de su longitud. El número de rachas total sería $R = \sum\limits_{i=1}^{n-1} R_i$. Usando el procedimiento anterior, se llega a que la distribución asintótica nula estandarizada con media $\mu = \frac{2n-1}{3}$ y varianza $\sigma^2=\frac{16n-29}{90}$ es la normal estándar.
	
\subsection{Test de bondad del ajuste}

	Una cuestión relevante en la estadística es la obtención de la forma de la población de la que se obtiene una muestra. En los test paramétricos, como se ha mencionado previamente, es habitual que la información relativa a la forma de la población se incluya entre las condiciones, por ejemplo en el test basado en la distribución $t$ de Student es necesaria la normalidad de la población. Por consiguiente, nos interesa disponer de test que nos indiquen cómo de confiable es la hipótesis de la normalidad para nuestra muestra. Los test de bondad del ajuste se ocupan de la forma de la población y no de la distribución concreta incluyendo parámetros de escala o localización.
	
\subsubsection{Chi cuadrado}

	Sea una muestra aleatoria de tamaño $n$ obtenida de una población con función de distribución desconocida $F_X$. El test tendrá como hipótesis nula
		\[ H_0: F_X(x) = F_0(x) \forall x \]
	con $F_0$ conocida contra
		\[ H_1: F_X(x) \neq F_0(x) \text{ para algún }  x \]
	Para realizar este test, los datos deben disponerse en categorías, bien mediante los valores que toma la variable para distribuciones discretas o mediante rangos especificados al realizar el experimento para distribuciones continuas. Una vez realizadas estas categorías podemos obtener las frecuencia esperada si la hipótesis nula es cierta de $F_0(x)$.\\
	
	Supongamos que disponemos de $n$ muestras clasificadas en $k$ categorías mutuamente excluyentes. Sea $f_i$ la frecuencia observada y $e_i$ la frecuencia esperada para cada muestra. El estadístico que definimos para la realización de este test es $Q = \sum\limits_{i=1}^k \frac{(f_i-e_i)^2}{e_i}$, para el cual estudiaremos su distribución asintótica.\\
	
	Sean $\theta_1, \dots, \theta_k$ las probabilidades de pertenencia a cada clase y $f_1, \dots, f_k$ los valores observados. La función de verosimilitud será:
	
	\[ L(\theta_1, \dots, \theta_k) = 
			\prod\limits_{i=1}^k \theta_i^{f_i},\			
	   \text{ con } f_i = 0, 1, \dots, n; \
	   \sum\limits_{i=1}^k \theta_i = 1, \
	   \sum\limits_{i=1}^k f_i = n
	 \]
	 
	 Podemos por tanto escribir la hipótesis nula de la siguiente forma:
	 
	 \[ H_0 = \theta_i = \theta_i^0,\ i = 1, \dots, k \]
	 
	 habiendo obtenido cada $\theta_i^0$ de $F_0$. El estimador de máxima verosimilitud es $\hat{\theta}_i = \frac{f_i}{n}$. Entonces el ratio de verosimilitud es
	 
	 \[ 
	 T = \frac{L(\hat{\omega})}{L(\hat{\Omega})}
	   = \frac{L(\theta_1^0, \dots, \theta_k^0)}
	   		{L(\hat{\theta}_1^0, \dots, \hat{\theta}_k^0)}
	   = \prod\limits_{i=1}^k
	   		\left( 
	 			\frac{\theta_i^0}{\hat{\theta_i}} 
	 		\right)^f_i
	 \]
	 
	 la distribución de la v.a. $-2 \log T$ puede ser aproximada por la distribución chi cuadrado. Debido a que estimamos $k-1$ parámetros (el parámetro restante se deduce de la restricción $\sum\limits_{i=1}^k \theta_i = 1$). Tenemos entonces
	 \[ -2 \log T = 
	 		-2 \sum\limits_{i=1}^k
	 			f_i \left(
	 					\log \theta_i^0 - \log \frac{f_i}{n}
	 				\right)
	 \]
	, y mostramos a continuación que esta expresión es equivalente asintóticamente a la expresión de $Q$.\\
	
	La serie de Taylor de $\log \theta_i$ centrada en $f_i/n$ es
	\[ \log \theta_i = 
			\log \hat{\theta}_i +
			(\theta_i - \hat{\theta}_i)
				\frac{1}{\hat{\theta}_i} +
			\frac{(\theta_i - \hat{\theta}_i)^2}{2!}
				\left(-\frac{1}{\hat{\theta}_i^2}\right) +
			\dots
	\]
	
	con lo que
	
	\begin{align*}	
	 \log \theta_i^0 - \log \frac{f_i}{n} & = 
			\left(
				\theta_i^0 - \frac{f_i}{n}
			\right) \frac{n}{f_i} -
			\left(
				\theta_i^0 - \frac{f_i}{n}
			\right)^2 \frac{n^2}{2f_i^2} + \epsilon \\
		&= \frac{n\theta_i^0 - f_i}{f_i} -
			\frac{(n\theta_i^0 - f_i)^2}{2f_i^2} +\epsilon
	\end{align*}
	
	con $\epsilon = \sum\limits_{j=3}^\infty
			(-1)^{j+1} 
			\left( \theta_i^0 - \frac{f_i}{n}\right)^j
			\frac{n^j}{j!f_i^j}$. Sustituyendo en la expresión para $-2 \log T$ nos queda
			
	\begin{align*}
	-2 \log T &= 
		-2 \sum\limits_{i=1}^k n\theta_i^0 - f_i -
		\sum\limits_{i=1}^k 
			\frac{(n\theta_i^0 - f_i)^2}{f_i} + 
		\sum\limits_{i=1}^k \epsilon' = \\
	&= 0 + 
	   \sum\limits_{i=1}^k \frac{(f_i-e_i)^2}{f_i} +
	   \epsilon''	
	\end{align*}	 
	
	Por la ley de los grandes números, $F_i/n$ es un estimador consistente de $\theta_i$, es decir
	
	\[ \lim_{n \rightarrow \infty} \left[
			P \left(
				\vert \frac{F_i}{n} - \theta_i \vert 
					> \varepsilon
			\right) \right] = 0 \forall \varepsilon > 0 \]
	
	\subsubsection{Kolmogorov-Smirnov}
	
	En el test anterior sólo se realizaban $k$ comparaciones a pesar de disponer de $n$ ($n \geq k$) observaciones. Si las $n$ muestras provienen de una distribución continua, podríamos realizar la comparación entre la función de distribución que constituye la hipótesis nula y la \textbf{función de distribución empírica} (edf) $S_n$ definida de la siguiente forma:
	\[ S_n(x) = \frac{\text{número de muestras} \leq x}{n} \]
	Para una definición formal introducimos notación sobre estadísticos ordinales. Sea $X_1, \dots, X_n$ una muestra aleatoria de una población con función de distribución continua $F_X$. Sea $X_{(1)}$ el menor valor de $X_1, \dots, X_n; X_{(2)}$ el segundo menor;$ \dots; X_{(n)}$ el mayor. Entonces se puede definir $S_n(x)$ como:
	\[ S_n(x) = 
		\left\lbrace\begin{array}{ll}
			0 & \text{si } x < X_{(1)} \\
			i/n & \text{si } X_{(i)} < x <X_{(i+1)},
				i = 1, \dots, n-1 \\
			1 & \text{si } x > X_{(n)} \\
	\end{array}\right.
	\]
	
	$S_n(x)$ es un estimador consistente para $F_X(x)$ y conforme $n$ crece, se aproxima a $F_X(x)$ para todo $x$. Entonces cabe esperar que el error vaya disminuyendo. Se define el estadístico
	\[ D_n = \underset{x}{\sup} 
				\vert S_n(x) - F_0(x) \vert, \]
	que si la hipótesis nula es cierta es un buen indicador de la precisión de la estimación. El estadístico $D_n$ es especialmente útil en la inferencia no paramétrica debido a que la distribución de $D_n$ no depende de $F_0(x)$ mientras sea continua. Se definen las desviaciones direccionales
	\[ D_n^+ = \underset{sup}{x} [S_n(x) - F_0(x)]
		D_n^- = \underset{sup}{x} [F_0(x) - S_n(x)]\]
	
\begin{teorema}
	Los estadísticos $D_n, D_n^+$ y $D_n^-$ don libres de distribución para cualquier función de distribución continua $F_0$.
\end{teorema}
	
\begin{teorema}
	Si $F_X$ es una función de distribución continua, entonces para cada $d>0$
	\[ \underset{lim}{n \rightarrow \infty}
			P(D_n \leq d/\sqrt{n}) = L(d) \]
	con
	\[ L(d) = 1 - 2 \sum\limits_{i=1}^\infty 
			(-1)^{i-1} e^{-2i^2d^2}	\]
\end{teorema}

\begin{teorema}
	Si $F_0$ es una función de distribución continua, entonces bajo $H_0$ para cada $d>0$
	\[ \underset{lim}{n \rightarrow \infty}
			P(D_n^+ < d/\sqrt{n}) = 1-e^{-2d^2} \]
\end{teorema}	
	Como consecuencia de este teorema podemos usar las tablas de la distribución chi cuadrado:
	
\begin{corolario}
	Si $F_0$ es una función de distribución continua, entonces para cada $d \geq 0$, la distribución límite de $V = 4n {D_n^+}^2$ para $n \rightarrow \infty$ es la distribución chi cuadrado con dos grados de libertad.
\end{corolario}	
	
	
\subsection{Test basados en una muestra y en muestras emparejadas}
	
	En este apartado presentaremos los test no paramétricos análogos a test paramétricos como el test $t$ de Student para las hipótesis nulas $H_0: \mu = \mu_0$ para una única muestra y $H_0: \mu_X - \mu_Y = \mu_0$ para muestras emparejadas. En los test no paramétricos sólo serán necesarias condiciones sobre la continuidad de la población. 
	
\subsubsection{Test de signo}

	Supongamos una muestra de $n$ elementos de una población $F_X$ con mediana desconocida $M$, donde suponemos que $F_X$ es continua y estrictamente creciente al menos en el entorno de $M$. Esto significa que $F_X^{-1}(0.5) = M$. La hipótesis nula a comprobar se corresponde con el valor de la mediana:
	\[ H_0: M = M_0	\text{ o equivalentemente }
			\theta = P(X > M_0) = 0.5 \]
	con $M_0$ un valor dado. Como hemos supuesto que $F_X$ tiene una única mediana, la hipótesis significa que $M_0$ divide el área de la función de densidad en dos partes iguales. Denotamos por $K$ el número de observaciones mayores que $M_0$. Podemos considerar entonces que estamos obteniendo una muestra de una v.a. $K$ que sigue una distribución de Bernouilli con parámetros $n$ y $\theta=P(X>M_0)$, y $\theta=0.5$ si la hipótesis nula es cierta. Se llama test de signo debido a que $K$ es el número de signos positivos en $X_i - M_0, i = 1, \dots, n$. La hipótesis alternativa queda por tanto
	\[ H_1: M \neq M_0	\text{ o }
			\theta = P(X > M_0) \neq 0.5 \]
	La región de rechazo, para $\alpha$ el nivel de significación, es $K \geq k_{\alpha/2}$ ó $K \leq k_{\alpha/2}'$, con $k_{\alpha/2}$ y $k_{\alpha/2}'$ el menor y el mayor entero respectivamente tales que
	\[ \sum\limits_{i=k_{alpha/2}}^n
			{n \choose i}(0.5)^n \leq \frac{\alpha}{2}
		\text{ y }
		\sum\limits_{i=0}^{k_{alpha/2}'}
			{n \choose i}(0.5)^n \leq \frac{\alpha}{2}
	\]

\subsubsection{\textit{Signed-Rank test} de Wilcoxon}

	El test anterior usa únicamente el signo de la diferencia de la muestra a la mediana, sin considerar la distancia. Para este test sí consideramos la distancia a la mediana aunque necesitamos suponer la simetría de la población. \\
	Sea $X_1, \dots, X_n$ una muestra de una función de distribución continua $F$ con mediana $M$. De ser cierta la hipótesis nula $H_0: M = M_0$ las diferencias $D_i = X_i - M_0$ estarían distribuidas de manera simétrica en torno a 0.\\
	Supongamos que ordenamos las diferencias absolutas $|D_1|, \dots, |D_n|$ del valor absoluto más pequeño al más grande y le asignamos los puestos $1, 2, \dots, n$. Sea $T^+$ el valor esperado de la suma de los puestos con diferencias positivas, $T^-$ la de aquellos con diferencias negativas. Supuesta cierta la hipótesis nula, al ser la población simétrica, $T^+ = T^-$. Para realizar el test, notaremos por$r(\cdot)$ la función que asigna el puesto de la v.a.. Definimos
	\[ T^+ = \sum\limits_{i=1}^n Z_i r(|D_i|) \;
	   T^- = \sum\limits_{i=1}^n (1-Z_i) r(|D_i|) \]
	   
	 con $ Z_i = \lbrace \begin{array}{ll}
	 				1 & \text{si } D_i > 0 \\
	 				0 & \text{si } D_i \leq 0
	 		\end{array}right.$. Entonces,
	 		
	\[ T^+  - T^- = 2 \sum\limits_{i=1}^n
					 Z_i r(|D_i|) - \frac{N(N+1)}{2} \]
	
	Bajo la hipótesis nula, $Z_i$ sigue una di stribución de Bernoulli con $E(Z_i) = 1/2$ y $var(Z_i) = 1/4$. Usando que $T^+$ es una combinación lineal de las variables $Z_i$, tenemos
	\[ E[ T^+ | H_0 ] = \sum\limits_{i=1}^n 
						\frac{r(|D_i|)}{2} 
					= \frac{n(n+1)}{4}			\]
	y
	
	\[ var( T^+ | H_0 ) = \sum\limits_{i=1}^n 
						\frac{[r(|D_i|)]^2}{4} 
					= \frac{n(n+1)(2n+1)}{24}	\]
					
	Para la aplicación de este test se obtiene el estadístico $T = \min \{ T^+, T^- \}$. En la tabla de los valores críticos de $T$ para el test \textit{signed-rank} de Wilcoxon \cite[Tabla A5]{SHESKIN11} se encuentran aquellos valores para nivel de significación 0.05 y 0.01 para los cuales debemos rechazar la hipótesis nula si $T$ es menor que el valor correspondiente en la tabla.\\
	
\paragraph{Valores iguales a la mediana} En este test se han considerado los valores iguales a la mediana como negativos. Como las diferencias con la mediana están ordenadas de manera creciente, estos valores estarán necesariamente al principio y el impacto será menor. Sin embargo, podemos considerar la siguiente modificación,

	\[ Z_i = \lbrace \begin{array}{ll}
	 				1 & \text{si } D_i > 0 \\
	 				0 & \text{si } D_i < 0 \\
	 				1/2 & \text{si } D_i = 0 
	 		\end{array}right. 					\]
	
	para repartir así entre $T^+$ y $T^-$ estas puntuaciones. 
	
\subsubsection{Tratamiento de empates}
	
	Al realizar la suposición de que la muestra se obtiene de una población continua, la probabilidad teórica de obtener dos valores idénticos es nula. Sin embargo en la práctica podemos obtener empates debido a que la población sea discreta o limitaciones en la precisión. Presentamos a continuación algunos métodos para solventar esta situación:
	
\paragraph{Aleatorización} Se selecciona un orden aleatorio para los elementos con igual valor.

\paragraph{\textit{Midranks}} Asigna a cada individuo de un grupo de valores empatados la media de los puestos de la clasificación que tendrían de ser distintos. Es decir, si hay tres valores con la tercera menor distancia a la mediana, ocuparían los puestos 3,4 y 5 y por tanto el valor asignado a cada uno de ellos sería $\frac{3+4+5}{3}=4$. Es el método más utilizado debido a su simplicidad. 

\paragraph{Estadístico medio} Se realiza el test estadístico para todas las posibles asignaciones para los términos empatados y se realiza la media de estos test.

\paragraph{Estadístico menos favorable} Habiendo encontrado todos los posibles valores del test, se escoge aquel con menor probabilidad de rechazar la hipótesis nula. Es la opción más conservadora al minimizar la probabilidad de cometer un error de tipo I.

\paragraph{Rango de probabilidad} Se devuelve el valor menos favorable a rechazar la hipótesis nula y el más favorable. No conduce a una única decisión a no ser que ambos valores caigan en la región de rechazo o fuera de ella.

\paragraph{Omisión de los valores empatados} Otra posibilidad es descartar los valores empatados. Conlleva una pérdida de información y generalmente introduce un sesgo hacia el rechazo de la hipótesis nula.
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	



