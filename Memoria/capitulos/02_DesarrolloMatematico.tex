\chapter{Desarrollo matemático}

\section{Introducción a la inferencia}


	En esta primera sección se hace un breve repaso de los conceptos estadísticos necesarios para comprender el contenido de la memoria así como presentar la notación.
	
\begin{definicion}[Inferencia estadística]
	Rama de la estadística en la que se usan las propiedades de una muestra para extraer conclusiones de la población. 
\end{definicion}

\begin{definicion}[Variable aleatoria]
	Función de conjuntos cuyo dominio es los elementos de un espacio muestral sobre el cual se ha definido una función de probabilidad y cuyo rango es $\mathbb{R}$.\\
	$X$ es una variable aleatoria (v.a.) si para $x \in \mathbb{R}$ existe una probabilidad de que el valor tomado por la variable aleatoria sea menor o igual que $x$, es decir, $P(X \leq x) = F_X (x)$, llamada función de distribución de probabilidad (\textit{cumulative distribution function}, cdf) de $X$.	
\end{definicion}

Cualquier función de distribución, $F_X(x)$, de una v.a. $X$ cumple las siguientes propiedades: 
\begin{enumerate}
	\item $F_X$ es no decreciente: 
			$F_X(x_1) \leq F_X(x_2) \ \forall x_1 \leq x_2$.
	\item $\underset{x \rightarrow -\infty}{\lim} F_X(x) = 0$,
			$\underset{x \rightarrow \infty}{\lim} F_X(x) = 1$
	\item $F_X(x)$ es continua por la derecha: 
		$\underset{\varepsilon \rightarrow 0^+}{\lim} F_X(x+\varepsilon) = F_X(x)$
\end{enumerate}

	Diremos que una v.a. es \textbf{continua} si su cdf es continua. Supondremos que una cdf continua es derivable cpd (\textit{casi por doquier}, es decir, en todo $\mathbb{R}$ salvo en un conjunto finito de puntos).
	
\begin{definicion}[Función de densidad]
	Se define la función de densidad como la derivada de $F_X(x)$, $f_X(x)$. Para $X$ continua:
	\[ F_X(x) = \int_{-\infty}^x f_X(t) dt; \;
		f_X(x) = \frac{d}{dx}F_X(x) = F_X'(x) \geq 0; \;
		\int_{-\infty}^{\infty} f_X(x) dx = 1 \]
\end{definicion}
	
\begin{definicion}[Función de masa]
	Se define la función de masa de probabilidad  (\textit{probability mass function}, pmf) de una v.a. \textbf{discreta}, es decir, que sólo toma un número contable de valores como
	\[ 
	f_X(x) = P(X=x) = 
		F_X(x) - 
		\lim_{\varepsilon \rightarrow 0^+} F_X(X-\varepsilon)
	\]
\end{definicion}

\begin{definicion}[Muestra aleatoria]
	Llamamos muestra aleatoria de una v.a. $X$ a un conjunto de $n$ v.a., $(X_1, \dots, X_n)$, si son independientes e idénticamente distribuidas (i.i.d.), con lo que su distribución de probabilidad conjunta es
	\[ 
		f_{X}(x_1, \dots, x_n) =
		f_{X_1, \dots, X_n}(x_1, \dots, x_n) =
		\prod\limits_{i=1}^n f_X(x_i)
	\]
\end{definicion}
	
\begin{definicion}[Momento]
	Es un parámetro de la población. El momento $k$-ésimo de $X$ es $\mu_k' = E[X^k]$. La media es el momento de primer orden, $\mu_1' = E[X] = \mu$. El momento $k$-ésimo central es $\mu_k = E[(X - \mu)^k]$
\end{definicion}

\begin{teorema}[Teorema central del límite]
	Sea $X_1, \dots, X_n$ una muestra aleatoria de una población con media $\mu$ y varianza $\sigma^2 > 0$ y sea $\bar{X}_n$ la media de esa muestra. Entonces para $n \rightarrow \infty$ la variable aleatoria $\sqrt{n} \frac{(\bar{X}_n - \mu)}{\sigma}$ tiene como distribución límite la normal con media $0$ y varianza $1$.
\end{teorema}

\begin{definicion}[Estimador]
	Definimos como estimador, o estimador puntual una función de v.a. cuyo valor observado es usado para estimar el valor verdadero de un parámetro de la población. 
\end{definicion}

	Sea $\hat{\theta}_n = u(X_1, \dots, X_n)$ un estimador de un parámetro $\theta$. Incluimos unas propiedades deseables de $\hat{\theta}_n$:
	
	\begin{enumerate}
	\item \textit{Insesgadez}: 
			$E[\hat{\theta}_n] = \theta$ para todo $\theta$.
	\item \textit{Suficiencia}: Podemos escribir
			$f_{X_1, \dots, X_n}(x_1, \dots, x_n; \theta)$ como producto de dos funciones $f_{X_1, \dots, X_n}(x_1, \dots, x_n; \theta) = g(\hat{\theta}_n; \theta) H((x_1, \dots, x_n)$ tal que $H(x_1, \dots, x_n)$ no depende de $\theta$.
	\item \textit{Consistencia}
		\[ \lim_{n \rightarrow \infty} P(|\bar{\theta}_n - \theta| < \varepsilon) = 0 \quad \forall \varepsilon > 0 \]
	\begin{enumerate}
		\item Si $\hat{\theta}_n$ es un estimador insesgado de $\theta$ y $\underset{n \rightarrow \infty}{\lim} var(\hat{\theta}_n) = 0$, entonces $\hat{\theta}_n$ es un estimador consistente por la desigualdad de Chebyshev.
		\item $\hat{\theta}_n$ es un estimador consistente de $\theta$ si la distribución límite es la distribución degenerada con probabilidad $1$ en $\theta$.
	\end{enumerate}
			
	\item \textit{Mínimo error cuadrático} 	$E[(\hat{\theta}_n - \theta)^2] \leq E[(\hat{\theta}_n^* - \theta)^2]$ para cualquier estimador $\hat{\theta}_n^*$.
	\item \textit{Mínima varianza} 	$var(\hat{\theta}_n) \leq var(\hat{\theta}^*_n)$ para cualquier estimador $\hat{\theta}_n^*$, siendo ambos insesgados.
	\end{enumerate}
	
\begin{definicion}[Función de verosimilitud]
	La función de verosimilitud (\textit{likelihood function}) de una muestra aleatoria de tamaño $n$ de la población $f_X(x;\theta)$ es la probabilidad conjunta de las muestras tomadas como función de $\theta$. Esto es:
	\[ L(x_1, \dots, x_n; \theta) = 
		\prod\limits_{i=1}^n f_X(x_i;\theta)	\]
\end{definicion}

	Un \textbf{estimador máximo verosímil} (MLE) de $\theta$ es un valor $\bar{\theta}$ tal que 
	\[ L(x_1, \dots, x_n; \bar{\theta}) \geq 
			L(x_1, \dots, x_n; \theta) \ \forall \theta 	\]
	La relevancia de este estimador consiste en que, para unas ciertas condiciones de regularidad, un estimador máximo verosímil es suficiente, consistente y asintóticamente insesgado, con varianza mínima y con distribución normal.
	
	
\begin{definicion}[Intervalo de confianza]
	Un intervalo de confianza al $100(1-\alpha)\%$ para el parámetro $\theta$ es un intervalo aleatorio de extremos $U$ y $V$ (funciones de v.a.) tal que $P(U < \theta < V) = 1-\alpha$.
\end{definicion}
	
\begin{definicion}[Hipótesis estadística]
	Es una afirmación sobre la la función de probabilidad de una o más v.a. o una afirmación sobre las poblaciones de las cuales se han obtenido una o más muestras aleatorias. La \textbf{hipótesis nula}, $H_0$ es la hipótesis sobre la que se realizará un test. La \textbf{hipótesis alternativa}, $H_1$ es la que conclusión alcanzada si se rechaza la hipótesis nula.
\end{definicion}

\begin{definicion}[Región crítica]
	Llamamos región crítica o región de rechazo $R$ para un test al conjunto de valores tomados por el test que conducen a rechazar la hipótesis nula. Llamamos \textbf{valores críticos} a los extremos de $R$.
\end{definicion}

\begin{definicion}[Tipos de error]\textit{}
	\begin{description}
	\item[Error de tipo I] La hipótesis nula es rechazada siendo cierta.
	\item[Error de tipo II] La hipótesis nula no es rechazada siendo falsa.
	\end{description}
\end{definicion}

	Siendo $T$ un test estadístico con hipótesis $H_0: \theta \in \omega, \ H_1: \theta \in \Omega \setminus \omega$, los errores de tipo I y II tienen probabilidad
	\[ 
	\alpha(\theta) = P(T \in R | \theta \in \omega); \quad
	\beta(\theta) = 
		P(T \not\in R | 
				\theta \in \Omega \setminus \omega)
	\]
	respectivamente.

\begin{definicion}[Tamaño del test]
	Se define el tamaño del test como $\underset{\theta \in \omega}\sup \alpha(\theta)$.
\end{definicion}

\begin{definicion}[Potencia del test]
	Se define la potencia del test como la probabilidad de que el test conduzca a un rechazo de $H_0$: $Pw(\theta) = P(T \in R)$. Esta medida nos interesa cuando debemos rechazar la hipótesis nula, con lo que calculamos $Pw(\theta) = P(T \in R | \theta \in \Omega \setminus \omega) = 1 - \beta(\theta)$. 
\end{definicion}
	
	Diremos que un test es \textbf{más potente} para una hipótesis alternativa concreta si ningún test del mismo tamaño tiene mayor potencia contra la misma hipótesis alternativa.\\
	A continuación definimos una aproximación alternativa a la realización de test de hipótesis, especialmente relevante en los test no paramétricos. 
	
\begin{definicion}[$p$-valor]
	Probabilidad de obtener, siendo cierta la hipótesis nula $H_0$, una muestra aleatoria tan alejada o más de la hipotesis nula que la muestra aleatoria observada.
\end{definicion}

\begin{definicion}[Consistencia]
	Diremos que un test es consistente para una hipótesis alternativa $H_1$ si la potencia del test se aproxima a 1 conforme $n \rightarrow \infty$, siendo $n$ el tamaño de la muestra.
\end{definicion}
	
	
\section{Test paramétricos}

\section{Test no paramétricos}

	Para poder comprender este concepto, definimos la familia no paramétrica de distribuciones según \cite{PESSAL10}, sobre la que se realizan los test no paramétricos.
 
\begin{definicion}[Familia de distribuciones no paramétrica]
	Una familia de distribuciones $\mathcal{P}$ se dice que se comporta de manera no paramétrica cuando no es posible encontrar un espacio de dimensión finita $\Theta$ tal que hay una relación uno a uno entre $\Theta$ y $\mathcal{P}$, en el sentido en que cada elemento $P \in \mathcal{P}$ no puede ser identificado por un único elemento $\theta \in \Theta$ y viceversa.
\end{definicion}

	Si existiese esta relación, $\mathcal{P}$ sería una familia paramétrica y $\theta$ sería el parámetro.\\
	En base a esta definición, consideraremos la inferencia estadística no paramétrica aquella que se realiza sobre una muestra suponiendo la pertenencia de ésta a una familia no paramétrica de distribuciones.
	
	\subsection*{Comparación con test paramétricos}
	
	En la inferencia clásica se efectúan suposiciones sobre la población de la que se extraen muestras para realizar la inferencia. Aunque estas suposiciones están normalmente justificadas, en ocasiones no se dan las circunstancias necesarias para aplicar estas técnicas o su uso no está bien documentado. Por ello surgen las técnicas no paramétricas. Nótese que es distinta la suposición al aplicar estas técnicas no paramétricas, pues consideramos en este caso que no conocemos su distribución.\\
	
 	La principal ventaja de los test no paramétricos es que las hipótesis son más generales con lo que se pueden aplicar en un mayor número de problemas. Una de las condiciones habituales es la continuidad, aunque hay otras condiciones más estrictas como que la población sea simétrica para según qué test. Esto repercute en que se le pueden aplicar funciones a las muestras obtenidas para la realización de los test, a diferencia de en los test paramétricos dado que las muestras deben generalmente provenir de una población de forma conocida.\\
 	
 	Hay test de hipótesis que no están relacionados con valores de parámetros (a diferencia de los paramétricos). Son más simples de aplicar, las matemáticas, menos sofisticadas y basadas en la combinatoria, están relacionadas con las propiedades usadas en el proceso inductivo. Los libros de recetas no son necesarios, pues con la mera definición del test queda suficientemente claro cómo aplicar el test no paramétrico. Además, las distribuciones asintóticas son distribuciones conocidas como la normal o la chi cuadrado. Al relajar las condiciones sobre los datos de entrada, es menos sensible al \textit{dirty-data}, datos con errores usados en el entrenamiento del clasificador. Esto implica una mayor robustez en los test no paramétricos.\\
 	
 	Correctamente aplicados, los test paramétricos, al disponer de mayor información tienen una mayor potencia, sin embargo, cuando se disponen de menos datos, y por tanto es más difícil que se den las condiciones de los test paramétricos, la potencia es similar. Además, en esta situación, la falta de eficiencia de los test no paramétricos se suple con la falta de precisión de los test paramétricos al aproximar la distribución de los datos por la distribución asintótica conocida.


 	Se incluye en esta sección el desarrollo de los test no paramétricos más utilizados en aprendizaje automático.
 	
 	
\subsection{Test de aleatoriedad}

	Una de las condiciones para la realización de los test estadísticos, tanto de los paramétricos como de los no paramétricos, es la aleatoriedad de la muestra de partida. La hipótesis nula para los test que serán presentados en esta sección será la aleatoriedad de la muestra, mientras que la hipótesis alternativa será la presencia de un patrón. Estos test también son útiles en los estudios de series temporales y control de calidad.
	
\subsubsection{Test basado en el número de rachas}

	Supongamos una secuencia de $n$ elementos de dos tipos, $n_1$ del primer tipo y $n_2$ del segundo, $n = n_1 + n_2$. Sea $R_1$ el número de rachas del primer tipo, $R_2$ el número de rachas del segundo tipo, $R = R_1 + R_2$. Siendo cierta la hipótesis nula (la aleatoriedad de la muestra), procedemos a obtener la distribución de $R$.
	
\begin{lema} 
	El número de formas distintas de distribuir $n$ objetos en $r$ posiciones consecutivas es ${n-1 \choose r-1}, n \geq r, r \geq 1$.
\end{lema}

\begin{teorema}
	Sean $R_1$ y $R_2$ los números de rachas de los $n_1$ de tipo 1 y los $n_2$ elementos de tipo 2 respectivamente en una muestra de tamaño $n = n_1 + n_2$. La función de distribución de probabilidad conjunta de $R_1$ y $R_2$ es
	\[ f_{R_1,R_2} (r_1, r_2) = 
		\frac{c {n_1 - 1 \choose r_1 - 1} 
				{n_2 - 1 \choose r_2 - 1}}
			{{n_1 + n_2 \choose n_1}}\;
		\begin{array}{l}
			r_1 = 1,2, \dots, n_1 \\
			r_2 = 1,2, \dots, n_2 \\
			r_1 = r_2 \text{ ó } r_1 = r_2 \pm 1
		\end{array}
	\]
	donde $c=2$ si $r_1 = r_2$ (hay igual número de rachas de elementos del tipo 1 y del tipo 2) y $c=1$ si $r_1 = r_2 \pm 1$ (hay una racha más del tipo 1 ó 2).
\end{teorema}

	Para muestras de un mayor tamaño (aquellas en el que $n_1, n_2 \geq 10$) se suele utilizar una aproximación utilizando la distribución asintótica supuesto cierta $H_0$.\\
	Suponemos que el tamaño de la muestra $n \rightarrow \infty$, de forma en que $\frac{n_1}{n} \rightarrow \lambda$, $0<\lambda<1$. De aquí obtenemos
	\[ \underset{n \rightarrow \infty}{\lim} E[R/n] = 
			2\lambda (1-\lambda) 
				\underset{n \rightarrow \infty}{\lim} 
					var(R\sqrt{n}) =
			4\lambda^2(1-\lambda)^2
	\]
	
\begin{teorema}
	La distribución de probabilidad de $R$, es decir, el número total de rachas en una muestra aleatoria es:
	
	\begin{equation}
		f_R(r) = \left\lbrace\begin{array}{ll}
	2 {n_1-1 \choose r/2-1} {n_2-1 \choose r/2-1} 
		\big/ {n_1 + n_2 \choose n_1} &
			\textit{ si } r \text{ es par} \\
	\left[
		{n_1-1 \choose (r-1)/2} {n_2-1 \choose (r-3)/2} +  
		{n_1-1 \choose (r-3)/2} {n_2-1 \choose (r-1)/2} 
	\right]
		\big/ {n_1 + n_2 \choose n_1} &
			\textit{ si } r \text{ es impar} \\		
		\end{array}\right.
	\label{th-dist-R}
	\end{equation}
	para $r=2, 3, \dots, n_1 + n_2.$
\end{teorema}
	
	Si llamamos $Z = \frac{R - 2n\lambda (1-\lambda)}{2 \sqrt{n}\lambda (1-\lambda)}$ y sustituimos en \ref{th-dist-R}, obtenemos la distribución estandarizada de $R$, $f_Z(z)$. Entonces aplicamos la fórmula de Stirling y el límite queda de la forma
	\[ \underset{n \rightarrow \infty}{\lim} \log f_Z(z)=
			-\log \sqrt{2\pi} - \frac{1}{2} z^2	\]
	con lo que la distribución límite de $Z$ es la normal. 
	
	
\subsubsection{Test basado en rachas crecientes y decrecientes}	

	Para este test consideramos una serie de datos de tipo numérico ordenados temporalmente y queremos comprobar la hipótesis de la aleatoriedad de la muestra.\\
	Para una muestra de $n$ elementos, supongamos que podemos ordenarlos de la forma $a_1 < \dots < a_n$ (estamos suponiendo que no hay dos iguales. Si la hipótesis nula fuese cierta, nuestra muestra se corresponderá con una de las $n!$ permutaciones con igual probabilidad. Usaremos para este test las rachas crecientes y decrecientes. Construimos la secuencia $D_{n-1}$, cuyo elemento $i$-ésimo es el signo de $x_{i+1} - x_i,\ i=1, \dots, n-1$. Sean $R_1, \dots, R_{n-1}$ el número de rachas de longitud $1, \dots, n-1$ respectivamente. $f_n(r_{n-1}, \dots, r_1)$ indica la probabilidad de obtener $r_j$ rachas de longitud $j$ supuesta cierta la hipótesis nula. Escribiremos como $u_n$ la frecuencia absoluta $f_n = \frac{u_n}{n!}$. Para obtener la función de distribución, partiremos del caso $n=3$\\
	Sean $a_1 < a_2 < a_3$. La distribución de probabilidad será 
	\[ 
	f_3(r_2, r_1) = 
		\left\lbrace\begin{array}{cc}
			\frac{2}{6} & \text{si } r_2 = 1, r_1 = 0 \\
			\frac{4}{6} & \text{si } r_2 = 0, r_1 = 2 
		\end{array}\right.
	\]
	dado que las únicas rachas de longitud 2 son $(a_1, a_2, a_3) \rightarrow (+,+)$ y $(a_3, a_2, a_1) \rightarrow (-,-)$, siendo las demás posibles combinaciones dos rachas de longitud 1.\\
	Las posibilidades a la hora de insertar un elemento $a_n$ en las permutaciones de $S_n$ son:
\begin{enumerate}
	\item Se añade una racha de longitud 1.
	\item Una racha de longitud $i-1$ se convierte en una de longitud $i$, $i=2,\dots n-1$.
	\item Una racha de longitud $h=2i$ se convierte en una de longitud $i$, seguida por otra de longitud $1$, seguida por otra de longitud $i$.
	\item Una racha de longitud $h=i+j$ se convierte en
	\begin{enumerate}
		\item Una racha de longitud $i$ seguida por otra de longitud $1$ seguida por otra de longitud $j$.
		\item Una racha de longitud $j$ seguida por otra de longitud $1$ seguida por otra de longitud $i$.
	\end{enumerate}
	con $h>i>j$, $3 \leq h \leq n-2$
\end{enumerate}

	De forma general, la frecuencia $u_n$ conocido $u_{n-1}$ sigue la siguiente relación:
\begin{align*}
	& u_n (r_{n-1}, \dots, r_h, \dots, r_i, \dots, r_j, \dots, r_1)= 
		2 u_{n-1}(r_{n-2}, \dots, r_1-1) \\
	&+ \sum\limits_{i=2}^{n-1} 
		(r_{i-1} + 1)
		u_{n-1}(r_{n-2},\dots, r_i-1, r_{i-1}+1,\dots, r_1)\\
	&+ \sum\limits_{i=1, h=2i}^{\lfloor (n-2)/2 \rfloor} 
		(r_{h} + 1)
		u_{n-1}(r_{n-2},\dots, r_h+1,\dots r_i-2,\dots, r_1-1)\\
	&+ 2 \underbrace{\sum\limits_{i=2}^{n-3} \sum\limits_{j=1}^{i-1}}_{h=i+j, h \leq n-2}
		(r_{h} + 1)
		u_{n-1}(r_{n-2},\dots, r_h+1,\dots, r_i-1,\dots, r_1-1)			
\end{align*}
	
	Otro test que se podría realizar es el del número total de rachas, independientemente de su longitud. El número de rachas total sería $R = \sum\limits_{i=1}^{n-1} R_i$. Usando el procedimiento anterior, se llega a que la distribución asintótica nula estandarizada con media $\mu = \frac{2n-1}{3}$ y varianza $\sigma^2=\frac{16n-29}{90}$ es la normal estándar.
	
\subsection{Test de bondad del ajuste}

	Una cuestión relevante en la estadística es la obtención de la forma de la población de la que se obtiene una muestra. En los test paramétricos, como se ha mencionado previamente, es habitual que la información relativa a la forma de la población se incluya entre las condiciones, por ejemplo en el test basado en la distribución $t$ de Student es necesaria la normalidad de la población. Por consiguiente, nos interesa disponer de test que nos indiquen cómo de confiable es la hipótesis de la normalidad para nuestra muestra. Los test de bondad del ajuste se ocupan de la forma de la población y no de la distribución concreta incluyendo parámetros de escala o localización.
	
\subsubsection{Chi cuadrado}

	Sea una muestra aleatoria de tamaño $n$ obtenida de una población con función de distribución desconocida $F_X$. El test tendrá como hipótesis nula
		\[ H_0: F_X(x) = F_0(x) \ \forall x \]
	con $F_0$ conocida contra
		\[ H_1: F_X(x) \neq F_0(x) \text{ para algún }  x \]
	Para realizar este test, los datos deben disponerse en categorías, bien mediante los valores que toma la variable para distribuciones discretas o mediante rangos especificados al realizar el experimento para distribuciones continuas. Una vez realizadas estas categorías podemos obtener las frecuencia esperada si la hipótesis nula es cierta de $F_0(x)$.\\
	
	Supongamos que disponemos de $n$ muestras clasificadas en $k$ categorías mutuamente excluyentes. Sea $f_i$ la frecuencia observada y $e_i$ la frecuencia esperada para cada muestra. El estadístico que definimos para la realización de este test es $Q = \sum\limits_{i=1}^k \frac{(f_i-e_i)^2}{e_i}$, para el cual estudiaremos su distribución asintótica.\\
	
	Sean $\theta_1, \dots, \theta_k$ las probabilidades de pertenencia a cada clase y $f_1, \dots, f_k$ los valores observados. La función de verosimilitud será:
	
	\[ L(\theta_1, \dots, \theta_k) = 
			\prod\limits_{i=1}^k \theta_i^{f_i},\			
	   \text{ con } f_i = 0, 1, \dots, n; \
	   \sum\limits_{i=1}^k \theta_i = 1, \
	   \sum\limits_{i=1}^k f_i = n
	 \]
	 
	 Podemos por tanto escribir la hipótesis nula de la siguiente forma:
	 
	 \[ H_0 = \theta_i = \theta_i^0,\ i = 1, \dots, k \]
	 
	 habiendo obtenido cada $\theta_i^0$ de $F_0$. El estimador de máxima verosimilitud es $\hat{\theta}_i = \frac{f_i}{n}$. Entonces el ratio de verosimilitud es
	 
	 \[ 
	 T = \frac{L(\hat{\omega})}{L(\hat{\Omega})}
	   = \frac{L(\theta_1^0, \dots, \theta_k^0)}
	   		{L(\hat{\theta}_1^0, \dots, \hat{\theta}_k^0)}
	   = \prod\limits_{i=1}^k
	   		\left( 
	 			\frac{\theta_i^0}{\hat{\theta_i}} 
	 		\right)^{f_i}
	 \]
	 
	 y la distribución de la v.a. $-2 \log T$ puede ser aproximada por la distribución chi cuadrado. Debido a que estimamos $k-1$ parámetros (el parámetro restante se deduce de la restricción $\sum\limits_{i=1}^k \theta_i = 1$). Tenemos entonces
	 \begin{equation}
	 2 \log T = 
	 		-2 \sum\limits_{i=1}^k
	 			f_i \left(
	 					\log \theta_i^0 - \log \frac{f_i}{n}
	 				\right)
	 \label{2logT}
	 \end{equation}
	y mostramos a continuación que esta expresión es equivalente asintóticamente a la expresión de $Q$.\\
	
	La serie de Taylor de $\log \theta_i$ centrada en $\hat{\theta}_i = f_i/n$ es
	\[ \log \theta_i = 
			\log \hat{\theta}_i +
			(\theta_i - \hat{\theta}_i)
				\frac{1}{\hat{\theta}_i} +
			\frac{(\theta_i - \hat{\theta}_i)^2}{2!}
				\left(-\frac{1}{\hat{\theta}_i^2}\right) +
			\dots
	\]
	
	con lo que
	
	\begin{align*}	
	 \log \theta_i^0 - \log \frac{f_i}{n} & = 
			\left(
				\theta_i^0 - \frac{f_i}{n}
			\right) \frac{n}{f_i} -
			\left(
				\theta_i^0 - \frac{f_i}{n}
			\right)^2 \frac{n^2}{2f_i^2} + \epsilon \\
		&= \frac{n\theta_i^0 - f_i}{f_i} -
			\frac{(n\theta_i^0 - f_i)^2}{2f_i^2} +\epsilon
	\end{align*}
	
	con $\epsilon = \sum\limits_{j=3}^\infty
			(-1)^{j+1} 
			\left( \theta_i^0 - \frac{f_i}{n}\right)^j
			\frac{n^j}{j!f_i^j}$. Sustituyendo en \ref{2logT} nos queda
			
	\begin{align*}
	-2 \log T &= 
		-2 \sum\limits_{i=1}^k n\theta_i^0 - f_i -
		\sum\limits_{i=1}^k 
			\frac{(n\theta_i^0 - f_i)^2}{f_i} + 
		\sum\limits_{i=1}^k \epsilon' = \\
	&= 0 + 
	   \sum\limits_{i=1}^k \frac{(f_i-e_i)^2}{f_i} +
	   \epsilon''	
	\end{align*}	 
	
	Por la ley de los grandes números, $F_i/n$ es un estimador consistente de $\theta_i$, es decir
	
	\[ \lim_{n \rightarrow \infty} \left[
			P \left(
				\abs{ \frac{F_i}{n} - \theta_i } 
					> \varepsilon
			\right) \right] = 0 \; \forall \varepsilon > 0 \]
	
	\subsubsection{Kolmogorov-Smirnov}
	
	En el test anterior sólo se realizaban $k$ comparaciones a pesar de disponer de $n$ ($n \geq k$) observaciones. Si las $n$ muestras provienen de una distribución continua, podríamos realizar la comparación entre la función de distribución que constituye la hipótesis nula y la \textbf{función de distribución empírica} (edf) $S_n$ definida de la siguiente forma:
	\[ S_n(x) = \frac{\text{número de muestras} \leq x}{n} \]
	Para una definición formal introducimos notación sobre estadísticos ordinales. Sea $X_1, \dots, X_n$ una muestra aleatoria de una población con función de distribución continua $F_X$. Sea $X_{(1)}$ el menor valor de $X_1, \dots, X_n; X_{(2)}$ el segundo menor;$ \dots; X_{(n)}$ el mayor. Entonces se puede definir $S_n(x)$ como:
	\[ S_n(x) = 
		\left\lbrace\begin{array}{ll}
			0 & \text{si } x < X_{(1)} \\
			i/n & \text{si } X_{(i)} < x <X_{(i+1)},
				i = 1, \dots, n-1 \\
			1 & \text{si } x > X_{(n)} \\
	\end{array}\right.
	\]
	
	$S_n(x)$ es un estimador consistente para $F_X(x)$ y conforme $n$ crece, se aproxima a $F_X(x)$ para todo $x$. Entonces cabe esperar que el error vaya disminuyendo. Se define el estadístico
	\[ D_n = \underset{x}{\sup} 
				\vert S_n(x) - F_0(x) \vert, \]
	que si la hipótesis nula es cierta es un buen indicador de la precisión de la estimación. El estadístico $D_n$ es especialmente útil en la inferencia no paramétrica debido a que la distribución de $D_n$ no depende de $F_0(x)$ mientras sea continua. Se definen las desviaciones direccionales
	\[ D_n^+ = \underset{x}{\sup} [S_n(x) - F_0(x)]
		D_n^- = \underset{x}{\sup} [F_0(x) - S_n(x)]\]
	
\begin{teorema}
	Los estadísticos $D_n, D_n^+$ y $D_n^-$ son libres de distribución para cualquier función de distribución continua $F_0$.
\end{teorema}
	
\begin{teorema}
	Si $F_X$ es una función de distribución continua, entonces para cada $d>0$
	\[ \underset{n \rightarrow \infty}{lim}
			P(D_n \leq d/\sqrt{n}) = L(d) \]
	con
	\[ L(d) = 1 - 2 \sum\limits_{i=1}^\infty 
			(-1)^{i-1} e^{-2i^2d^2}	\]
\end{teorema}

\begin{teorema}
	Si $F_0$ es una función de distribución continua, entonces bajo $H_0$ para cada $d>0$
	\[ \underset{n \rightarrow \infty}{lim}
			P(D_n^+ < d/\sqrt{n}) = 1-e^{-2d^2} \]
\end{teorema}	
	Como consecuencia de este teorema podemos usar las tablas de la distribución chi cuadrado:
	
\begin{corolario}
	Si $F_0$ es una función de distribución continua, entonces para cada $d \geq 0$, la distribución límite de $V = 4n {D_n^+}^2$ para $n \rightarrow \infty$ es la distribución chi cuadrado con dos grados de libertad.
\end{corolario}	
	
	
\subsection{Test basados en una muestra y en muestras emparejadas}
	
	En este apartado presentaremos los test no paramétricos análogos a test paramétricos como el test $t$ de Student para las hipótesis nulas $H_0: \mu = \mu_0$ para una única muestra y $H_0: \mu_X - \mu_Y = \mu_0$ para muestras emparejadas. En los test no paramétricos sólo serán necesarias condiciones sobre la continuidad de la población. 
	
\subsubsection{Test de signo}

	Supongamos una muestra de $n$ elementos de una población $F_X$ con mediana desconocida $M$, donde suponemos que $F_X$ es continua y estrictamente creciente al menos en el entorno de $M$. Esto significa que $F_X^{-1}(0.5) = M$. La hipótesis nula a comprobar se corresponde con el valor de la mediana:
	\[ H_0: M = M_0	\text{ o equivalentemente }
			\theta = P(X > M_0) = 0.5 \]
	con $M_0$ un valor dado. Como hemos supuesto que $F_X$ tiene una única mediana, la hipótesis significa que $M_0$ divide el área de la función de densidad en dos partes iguales. Denotamos por $K$ el número de observaciones mayores que $M_0$. Podemos considerar entonces que estamos obteniendo una muestra de una v.a. $K$ que sigue una distribución de Bernouilli con parámetros $n$ y $\theta=P(X>M_0)$, y $\theta=0.5$ si la hipótesis nula es cierta. Se llama test de signo debido a que $K$ es el número de signos positivos en $X_i - M_0, i = 1, \dots, n$. La hipótesis alternativa queda por tanto
	\[ H_1: M \neq M_0	\text{ ó }
			\theta = P(X > M_0) \neq 0.5 \]
	La región crítica, para $\alpha$ el nivel de significación, es $K \geq k_{\alpha/2}$ ó $K \leq k_{\alpha/2}'$, con $k_{\alpha/2}$ y $k_{\alpha/2}'$ el menor y el mayor entero respectivamente tales que
	\[ \sum\limits_{i=k_{alpha/2}}^n
			{n \choose i}(0.5)^n \leq \frac{\alpha}{2}
		\text{ y }
		\sum\limits_{i=0}^{k_{alpha/2}'}
			{n \choose i}(0.5)^n \leq \frac{\alpha}{2}
	\]

\subsubsection{\textit{Signed-Rank test} de Wilcoxon}

	El test anterior usa únicamente el signo de la diferencia de la muestra a la mediana, sin considerar la distancia. Para este test sí consideramos la distancia a la mediana aunque necesitamos suponer la simetría de la población. \\
	Sea $X_1, \dots, X_n$ una muestra de una función de distribución continua $F$ con mediana $M$. De ser cierta la hipótesis nula $H_0: M = M_0$ las diferencias $D_i = X_i - M_0$ estarían distribuidas de manera simétrica en torno a 0.\\
	Supongamos que ordenamos las diferencias absolutas $|D_1|, \dots, |D_n|$ del valor absoluto más pequeño al más grande y le asignamos los puestos $1, 2, \dots, n$. Sea $T^+$ el valor esperado de la suma de los puestos con diferencias positivas, $T^-$ la de aquellos con diferencias negativas. Supuesta cierta la hipótesis nula, al ser la población simétrica, $T^+ = T^-$. Para realizar el test, notaremos por$r(\cdot)$ la función que asigna el puesto de la v.a.. Definimos
	\[ T^+ = \sum\limits_{i=1}^n Z_i r(|D_i|); \quad
	   T^- = \sum\limits_{i=1}^n (1-Z_i) r(|D_i|) \]
	   
	 con $ Z_i = \left\lbrace \begin{array}{ll}
	 				1 & \text{si } D_i > 0 \\
	 				0 & \text{si } D_i \leq 0
	 		\end{array} \right.$. Entonces,
	 		
	\[ T^+  - T^- = 2 \sum\limits_{i=1}^n
					 Z_i r(|D_i|) - \frac{N(N+1)}{2} \]
	
	Bajo la hipótesis nula, $Z_i$ sigue una di stribución de Bernoulli con $E(Z_i) = 1/2$ y $var(Z_i) = 1/4$. Usando que $T^+$ es una combinación lineal de las variables $Z_i$, tenemos
	\[ E[ T^+ | H_0 ] = \sum\limits_{i=1}^n 
						\frac{r(|D_i|)}{2} 
					= \frac{n(n+1)}{4}			\]
	y
	
	\[ var( T^+ | H_0 ) = \sum\limits_{i=1}^n 
						\frac{[r(|D_i|)]^2}{4} 
					= \frac{n(n+1)(2n+1)}{24}	\]
					
	Para la aplicación de este test se obtiene el estadístico $T = \min \{ T^+, T^- \}$. En la tabla de los valores críticos de $T$ para el test \textit{signed-rank} de Wilcoxon \cite[Tabla A5]{SHESKIN11} se encuentran aquellos valores para nivel de significación 0.05 y 0.01 para los cuales debemos rechazar la hipótesis nula si $T$ es menor que el valor correspondiente en la tabla.\\
	
\paragraph{Valores iguales a la mediana} En este test se han considerado los valores iguales a la mediana como negativos. Como las diferencias con la mediana están ordenadas de manera creciente, estos valores estarán necesariamente al principio y el impacto será menor. Sin embargo, podemos considerar la siguiente modificación,

	\[ Z_i = \left\lbrace \begin{array}{ll}
	 				1 & \text{si } D_i > 0 \\
	 				0 & \text{si } D_i < 0 \\
	 				1/2 & \text{si } D_i = 0 
	 		\end{array} \right. 					\]
	
	para repartir así entre $T^+$ y $T^-$ estas puntuaciones. 
	
\subsubsection{Tratamiento de empates}
	
	Al realizar la suposición de que la muestra se obtiene de una población continua, la probabilidad teórica de obtener dos valores idénticos es nula. Sin embargo en la práctica podemos obtener empates debido a que la población sea discreta o limitaciones en la precisión. Presentamos a continuación algunos métodos para solventar esta situación:
	
\paragraph{Aleatorización} Se selecciona un orden aleatorio para los elementos con igual valor.

\paragraph{\textit{Midranks}} Asigna a cada individuo de un grupo de valores empatados la media de los puestos de la clasificación que tendrían de ser distintos. Es decir, si hay tres valores con la tercera menor distancia a la mediana, ocuparían los puestos 3,4 y 5 y por tanto el valor asignado a cada uno de ellos sería $\frac{3+4+5}{3}=4$. Es el método más utilizado debido a su simplicidad. 

\paragraph{Estadístico medio} Se realiza el test estadístico para todas las posibles asignaciones para los términos empatados y se realiza la media de estos test.

\paragraph{Estadístico menos favorable} Habiendo encontrado todos los posibles valores del test, se escoge aquel con menor probabilidad de rechazar la hipótesis nula. Es la opción más conservadora al minimizar la probabilidad de cometer un error de tipo I.

\paragraph{Rango de probabilidad} Se devuelve el valor menos favorable a rechazar la hipótesis nula y el más favorable. No conduce a una única decisión a no ser que ambos valores caigan en la región crítica o fuera de ella.

\paragraph{Omisión de los valores empatados} Otra posibilidad es descartar los valores empatados. Conlleva una pérdida de información y generalmente introduce un sesgo hacia rechazar la hipótesis nula.
	
	
\subsection{Medidas de asociación en clasificaciones múltiples}	

	En este apartado presentamos la versión no paramétrica para el problema del análisis de la varianza. Enfocado al problema abordado en este trabajo, nos será muy útil para comparar el rendimiento de varios algoritmos en distintas bases de datos. Los datos se presentan en una tabla $n \times k$ con entradas $X_{ij}$. En esta distribución influyen dos factores que llamaremos el efecto por filas y por columnas.\\
	 Para nuestro problema, consideraremos que las filas se corresponden con bases de datos y que las columnas constituyen los algoritmos utilizados. Un enfoque similar sería considerar cada fila una porción de tierra y cada columna un tratamiento distinto aplicado a un cultivo.
	
\subsubsection{Análisis bidimensional de la varianza mediante clasificaciones en una tabla $k \times n$ y comparaciones múltiples de Friedman}

	Partimos de una matriz $n \times k$. Consideraremos que las filas son independientes, pero no las columnas (en nuestro problema esto significa que las bases de datos sí son independientes pero cada algoritmo los resultados guardan correlación). Friedman sugirió cambiar cada valor de la matriz por la clasificación de cada fila.
	
	\[ \left( \begin{matrix}
		R_{11} & R_{12} & \dots & R_{1k} \\
		\vdots & \vdots & \ddots & \vdots \\
		R_{n1} & R_{k2} & \dots & R_{nk}
		\end{matrix} \right)	\]

	Entonces, $R_{i1}, \dots, R_{ik}$ es una permutación de los $k$ primeros números (salvo en caso de empate). Escribimos como $R_j$ el total para la columna $j$. Consideraremos la distribución de la muestra de la v.a. 
	\begin{equation}
		 S = \sum\limits_{j=1}^k
				\left[
					R_j - \frac{n(k+1)}{2}
				\right]^2 =
			\sum\limits_{j=1}^k \left[
				\sum\limits_{i=1}^n \left(
					R_{ij} - \frac{k+1}{2}
				\right)
			\right]^2
	\label{S-Friedman}
	\end{equation}
	
	bajo la hipótesis nula $H_0: \theta_1 = \dots = \theta_k$. En el caso de que no hubiera empates por filas habría un total de $(k!)^n$ posibles entradas igualmente probables. Las posibilidades se pueden enumerar y calcular el valor de $S$ para cada una de ellas, con lo que la distribución de probabilidad de $S$ sería $f_S(s) = \frac{u_s}{(k!)^n}$, con $u_s$ el número de asignaciones que tienen como suma de los cuadrados de las desviaciones totales por columnas $s$. Existe un método para obtener $u_s$ de $k$ y $n$ a partir de $k$ y $n-1$, y tablas con valores. Para los valores que exceden estas tablas se usa una aproximación de la distribución.\\
	Notando $\mu = \frac{k+1}{2}$, escribimos \ref{S-Friedman}:
	
	\begin{align}
		S 	&= 	\sum\limits_{j=1}^k
					\sum\limits_{i=1}^n (R_{ij}-\mu)^2
				+ 2 \sum\limits_{j=1}^k
					\sum\limits_{i=1}^{n-1}
						\sum\limits_{p=i+1}^n
							(R_{ij}-\mu)(R_{pj}-\mu) 
				\nonumber \\
			&=	k \sum\limits_{j=1}^n (j-\mu)^2 + 2U 
				\nonumber \\
			&=  \frac{nk(k^2-1)}{12} + 2U	
	\label{S-Friedman2}
	\end{align}
	
	Los momentos de $S$ vienen determinados por los momentos de $U$, que se derivan de:
	\[ 	E[ R_{ij} ]= \frac{k+1}{2};\ 
		\var(R_{ij})=\frac{k^2-1}{12};\
		\cov(R_{ij},R_{iq}) = -\frac{k^2-1}{12}
	\]
	
	Además, dado que las observaciones en las diferentes filas son independientes, para $i \neq p$, la esperanza del producto de funciones de $R_{ij}$ y $R_{pq}$ estará multiplicado por $\cov(R_{ij},R_{pq})=0$. Entonces
	\[ E(U) = k {n \choose 2} \cov(R_{ij},R_{pq})=0 \] con lo que $var(U) = kE(U^2)$, con
	
	\begin{align*}
		U^2 =& \sum\limits_{j=1}^k
				\sum\limits_{1 \leq i < p \leq n}
					(R_{ij} - \mu)^2 (R_{pj} - \mu)^2 \\
			+& 2 
			    \sum\limits_{j=1}^{k-1}
			      \sum\limits_{q=j+1}^k
					\sum\limits_{i=1}^{n-1}
					  \sum\limits_{p=i+1}^n	
					  	\sum\limits_{r=1}^{n-1}
						  \sum\limits_{s=r+1}^n
				(R_{ij} - \mu)(R_{pj} - \mu)
				(R_{rq} - \mu)(R_{sq} - \mu)	
	\end{align*}
	
	Como $R_{ij}$ y $R_{pq}$ son independientes para $i \neq p$, tenemos
	\begin{align*}
		E[U^2] =& \sum\limits_{j=1}^k
					\sum\limits_{1 \leq i < p \leq n}
						var(R_{ij})var(R_{pj}) \\
			 &+ 2 
			    \sum\limits_{j=1}^{k-1}
			      \sum\limits_{q=j+1}^n
			    {n \choose 2}
				\cov(R_{ij}, R_{iq}) \cov(R_{pj}, R_{pq}) \\
			=& k {n \choose 2} \frac{(k^2-1)^2}{144} +
				2{k \choose 2}{n \choose 2} 
					\frac{(k^2+1)^2}{144} \\
			=& k^2 {n \choose 2} (k+1)^2 \frac{k-1}{144}
	\end{align*}	
	
	Sustituyendo en \ref{S-Friedman2} llegamos a 
	
	\[ E[S] = \frac{nk(k^2-1)}{12}; \ 
		\var(S)= \frac{k^2n(n-1)(k+1)^2}{72} .\]
	
	Podemos definir la transformación
	
	\[ \chi^2_F = \frac{12S}{nk(k+1)} = 
	\frac{12 \sum\limits_{j=1}^k R_j^2}{nk(k+1)} -3n(k+1) .
	\]
	
	que tiene $E[\chi^2_F] = k-1$, $\var(\chi^2_F)= 2(k-1)\frac{n-1}{n} \approx 2(k-1)$, que son los dos primeros momentos de la distribución $\chi^2_{k-1}$. También los momentos de mayor orden de $\chi^2_F$ se aproximan a los momentos de mayor orden de $\chi^2_{k-1}$ para $n$ grande. A efectos prácticos, podemos tratar $\chi^2_F$ como una $\chi^2_{k-1}$ para $n>7$ ó $n>10, k>5$. La región crítica para la hipótesis nula mencionada con nivel de significación $\alpha$ es
	\[ \chi^2_F \in R \ \text{ para } Q \geq \chi_{k-1,\alpha}^2 .\]
	
\subsection{Test basados en permutaciones}
	
	En este apartado se incluye una descripción general sobre los test basados en permutaciones. La hipótesis general a la hora de realizar inferencia es considerar que una muestra aleatoria $\mathbf{x}$ proviene de una misma población $ P \in \mathcal{P};\ H_0: \{ X \leadsto P \in \mathcal{P} \}$. Por tanto, supuesta esta hipótesis nula, obtenemos que si la muestra $\mathbf{x}$ viniese de poblaciones distintas, los valores pertenecientes a cada población podrían intercambiarse por los de otra sin ningún problema.\\
	Notemos además que $\mathbf{x}$ es un conjunto de estadísticos suficientes para cualquier distribución en $H_0$. Esto se debe a que, si $H_0$ es cierto, sea $f_P$ la densidad de $P$, $f_P^{(n)}(x)$ la densidad de la variable $X^(n)$. Como $f_P^{(n)}(\mathbf{x}) = f_P^{(n)}(\mathbf{x}) \cdot 1$ para todo $\mathbf{x} \in \Omega^n$, excepto para $f_P^{(n)}(\mathbf{x}) = 0$ debido al teorema de factorización cualquier conjunto $\mathbf{x}$ es un conjunto de estadísticos suficientes para cualquier $P \in \mathcal{P}$.\\
	En estos test se realizan inferencias condicionadas con respecto a la muestra $x$, que es a su vez un conjunto de estadísticos suficientes en $H_0$. Este hecho unido a la intercambiabilidad de los datos observados con respecto a los grupos hace que los test sean independientes de las distribución $P$, por lo que no necesitamos conocerla. Esto queda más claramente explicado en el siguiente principio:

\begin{definicion}[Principio de los test basados en permutaciones]
	Si dos experimentos toman valores en el mismo espacio muestral $\Omega$ respectivamente con las distribuciones $P_1, P_2 \in \mathcal{P}$, dan el mismo conjunto de datos $\mathcal{x}$, entonces las inferencias condicionales a $\mathcal{x}$ obtenidas usando el mismo test estadístico debe ser la misma, debido a la intercambiabilidad de los datos con respecto a los grupos se cumple en la hipótesis nula. Por consiguiente, si dos experimentos con distribuciones $P_1, P_2$ dan respectivamente como muestras $\mathbf{x_1}, \mathbf{x_2}$, con $\mathbf{x_1} \neq \mathbf{x_2}$, entonces las inferencias con respecto a $\mathbf{x_1}$ y $\mathbf{x_2}$ pueden ser distintas.
\end{definicion}


\subsubsection{Ejemplo con muestras emparejadas}

	Para explicar el funcionamiento de los test basados en permutaciones, expondremos situaciones ya conocidas en las que sabemos cómo utilizar test paramétricos o test no paramétricos expuestos anteriormente. El primer caso es el de muestras pareadas. Supondremos una muestra de 20 individuos que se somete a un tratamiento para reducir la ansiedad. Los datos se obtienen antes y después de realizar el experimento. Por tanto consideramos la v.a. $Y = (Y_1, Y_2)$ de $n=20$ elementos $\{(Y_{1i},Y_{2i}): i = 1, \dots, n\}$. La hipótesis nula es que las diferencias en que las distribuciones marginales de $Y_1$ e $Y_2$ son iguales: $H_0 : \{ Y_1 = Y_2 \}$ frente a $H_1: \{ Y_1 > Y_2 \}$\\
	
	En primer lugar, nótese que la hipótesis nula $H_0$ implica que las dos variables $Y_1, Y_2$ son intercambiables. Esto es, supuesto $H_0$, para cada individuo los dos valores observados podrían corresponder con cualquiera de las observaciones, antes o después del tratamiento. Es decir, si consideramos la variable aleatoria $X = Y_2 - Y_1$, cada valor observado $X_i, i=1, \dots, n$ podría tener cualquier signo con probabilidad $1/2$. Una forma de realizar el test sería considerando el estadístico $T = \sum\limits_{i=1}^n X_i$, cuya distribución condicionada a $\mathbf{X} = \{X_i: i = 1, \dots, n\}$, $F_T(t|\mathbf{X})$ vendrá dada bajo $H_0$ considerando la atribución de todas las formas posibles del signo de cada diferencia con probabilidad $1/2$. Nos referimos por $T^* = \sum\limits_{i=1}^n X_i^*$, donde $X_i^*$ se obtiene asignando aleatoriamente el signo. Notaremos por $\Omega_{/\mathbf{X}}$ al espacio muestral de permutaciones. Este espacio consiste en las posibles permutaciones de la muestra obtenidas, aunque posteriormente daremos una definición más formal. En este caso, el espacio tiene $M^{(n)} = 2^v$ elementos, donde $n-v$ es el número de diferencias nulas. Notamos por $F(t|\mathbf{X}) = P[T^* \leq t | \mathbf{X}], t \in \mathbb{R}$ a la cdf condicional de la permutación inducida por $T$ dado $\mathbf{X}$.\\
	Si el tamaño de la muestra $n$ es grande, podrá aplicarse el Teorema Central del Límite Permutacional (PCTL) para aproximar la distribución de la permutación $F[t|\mathbf{X}]$. Para resolver un problema de este tipo podemos guiarnos por lo siguiente:
	
\begin{enumerate}[a]
	\item Si $n \leq 25$ es posible calcular exactamente $T*$ para todas las posibles permutaciones y calcular $F[t|\mathbf{X}]$.
	\item Si $n \geq 200$, $\sigma_X$ se supone finito y el cociente $(\sum_i X_i^4)/(\sum_i X_i^2)^2$ es pequeño, $F[t|\mathbf{X}]$ puede aproximarse por el PCTL. 
	\item En otro caso, $F[t|\mathbf{X}]$ puede aproximarse utilizando el algoritmo condicional de Monte Carlo (CMC).
\end{enumerate}
	
\subsubsection{Teoría de los test basados en permutaciones}

\begin{definicion}[Espacio de referencia condicional]
	Conjunto de puntos del espacio muestral $\Omega^n$ que son equivalentes a $\mathbf{X}$ en términos de la información asociada a la verosimilitud subyacente. Lo notamos por $\Omega^n_{/\mathbf{X}}$.
\end{definicion}

	Esto es, $\Omega^n_{/\mathbf{X}}$ contiene todos los puntos $\mathbf{X}^*$ tales que $dP(X)/dP(X^*)$ es independiente de $P$. Dado que en $H_0$ la probabilidad de obtener $\mathbf{X}$ es la misma que de obtener una permutación $\mathbf{X}^*$,
	\[ 
		\Omega^n_{/\mathbf{X}} = 
			\left\lbrace
				\underset{\mathbf{u}^*}{\bigcup}
					[ X(u_i^*), i = 1, \dots, n ]
			\right\rbrace ,
	\]
	con $\mathbf{u}^*$ cualquier permutación de las etiquetas $(1, \dots, n)$.

\paragraph{Definicion de los test basados en permutaciones}
\begin{definicion}[Soporte de la permutación]
	Para realizar un test estadístico es necesaria una función no degenerada $T: \Omega^n \rightarrow \mathbf{R}$. Definimos el soporte de la permutación de $(T, \mathbf{X})$ como $\mathcal{T}_\mathbf{X} = \{ T^* = T(\mathbf{X}*): \mathbf{X}* \in \Omega_{/\mathbf{X}}\}$ 
\end{definicion}	

	Supongamos $H_0$ cierta. Entonces, $\mathbf{X}^*$ se distribuye uniformemente sobre $\Omega_{/\mathbf{X}}$. Pongamos los $M^{(n)}$ elementos de $\mathcal{T}_\mathbf{X}$ en orden creciente. De esta manera para $\alpha \in (0,1)$, se define $T_\alpha(\mathbf{X}) = T_\alpha = T^*_{(M^{(n)}_\alpha)}$ como el valor crítico asociado a $(T, \mathbf{X})$, donde $M^{(n)}_\alpha$ es el número de valores $T^*$ estrictamente menor que $T_\alpha$. Nótese que $T_\alpha$ depende de $\Omega_{/\mathbf{X}}$ y no únicamente de $\mathbf{X}$, pues $T_\alpha(\mathbf{X}) = T_\alpha(\mathbf{X}')$ para $\mathbf{X}' \in \Omega_{/\mathbf{X}}$. Esto implica que para $\alpha$ fijo, $T_\alpha$ es fijo para $\mathcal{T}_\mathbf{X}$.
	
\paragraph{Test basados en permutaciones aleatorizados}

	Se define la versión aleatorizada del test basado en permutaciones para $(T, \mathbf{X})$ como 
	\[ 
		\phi_R = \left\lbrace \begin{array}{cc}
			1 		& \text{ si } T^0 > T_\alpha \\
			\gamma 	& \text{ si } T^0 = T_\alpha \\
			0		& \text{ si } T^0 < T_\alpha \\
		\end{array} \right. ,
	\]
	
	donde $T^0 = T(\mathbf{X})$, el valor de $T$ en los datos observados y $\gamma$ es:
	\[
		\gamma = \frac{\alpha - P[T^0 > T_\alpha | 
								\Omega_{/\mathbf{X}}]}
					  {P[T^0 = T_\alpha | 
								\Omega_{/\mathbf{X}}]} .
	\]
	
	Para aplicar $\phi_R$ es habitual usar un experimento aleatorio independiente de $\mathbf{X}$. Por ejemplo, rechazando $H_0$ si $U \geq \gamma$ para $T^0 = T_\alpha$, con $U$ un valor aleatorio de la variable uniforme $\mathcal{U}(0,1)$.\\
	Para $\mathbf{X} \in \Omega^n$ la esperanza en $H_0$ de $\phi_R$ para $\alpha$ es:
	\[ 
		E[ \phi_R(\mathbf{X}) | \Omega_{/\mathbf{X}}] =
			P[ T^0(\mathbf{X}) > T_\alpha(\mathbf{X}) | 
								\Omega_{/\mathbf{X}}] + 
			P[ T^0(\mathbf{X}) = T_\alpha(\mathbf{X}) | 
								\Omega_{/\mathbf{X}}] =
			\alpha .
	\]
	
	Por consiguiente, el tamaño del test basado en permutaciones aleatorizado es exactamente $\alpha$.
	
\begin{proposicion}
	Supuesta la condición de intercambiabilidad para $\mathcal{X}$. Entonces para todas las distribuciones $P$ y uniformemente para todos los conjuntos de datos $\mathbf{X} \in \Omega^n$ la probabilidad condicionada de rechazo de $\phi_R$ es invariante con respecto a $\mathbf{X}$ y $P$.
\end{proposicion}

\paragraph{Test basados en permutaciones no aleatorizados}

	En los contextos de aplicación se suele utilizar la versión no aleatorizada. Se define
	
	\[ 
		\phi = \left\lbrace \begin{array}{cc}
			1 		& \text{ si } T^0 \geq T_\alpha \\
			0		& \text{ si } T^0 < T_\alpha \\
		\end{array} \right. .
	\]
	
	Ahora, el error asociado de tipo I es 
	
	\[ 
		E[ \phi(\mathbf{X}) | \Omega_{/\mathbf{X}}] =
		P[ T^0 \geq T_\alpha | \Omega_{/\mathbf{X}}] =
		\sum\limits_{\Omega_{/\mathbf{X}}}
			\mathbb{I}[ T(\mathbf{X}^*) \geq T_\alpha ]/
				M^{(n)} =
		\alpha_a \geq
		\alpha,
	\] 
	
	donde $\alpha_a$ es el llamado $\alpha$-valor alcanzable asociado a $(T, \mathbf{X})$. Para $(T,\mathbf{X})$, el $\alpha$-valor alcanzable pertenece a $\Lambda_\mathbf{X}^{(n)} = \{ L_\mathbf{X}(t): dL_\mathbf{X}(t) > 0 \}$, para la función $ L_\mathbf{X}(t) = P[ T^* \geq t | \Omega_{/\mathbf{X}}] $. 
	
\paragraph{$p$-Valor} Determinar el valor crítico $T_\alpha$ para un test estadístico $T$ puede ser complicado en la práctica. Por tanto, normalmente se hace referencia a un $p$-valor asociado a $(T, \mathbf{X})$, que se define como $p = \lambda_T(\mathbf{X}) = L_\mathbf{X}(T^0) = P[ T^* \geq T^0 | \Omega_{/\mathbf{X}}]$. Este valor se puede calcular enumerando completamente $\mathcal{T}_\mathbf{X}$ o estimándolo mediante el algoritmo condicional de Monte Carlo. El $p$-valor $p$ es una función creciente de $T^0$ y tiene una  correspondencia uno a uno con los $\alpha$-valores alcanzables de $\phi$, puesto que $\lambda_T(\mathbf{X}) > \alpha$ implica $T^0 < T_\alpha$ y viceversa.

\paragraph{Algoritmo CMC para estimar el $p$-valor}  Describimos a continuación los pasos generales de un algoritmo condicional de Monte Carlo para la evaluación de un $p$-valor de un test estadístico $T$ en un conjunto de datos $\mathbf{X} = \{ X_i, \ i=1, \dots, n; n_1, n_2 \}$

\begin{algorithm}
	\caption{Algoritmo CMC para estimar el $p$-valor}
	\label{alg:CMC-pvalue}
	\begin{algorithmic}[1]
	\REQUIRE
		\begin{enumerate}[a]
		\item Muestra $\mathbf{X}$
		\item Test estadístico $T$
		\item Número de iteraciones $B$
		\end{enumerate}
		\STATE Calcular $T^0 = T(\mathbf{X})$
		\FOR { $b \in \{1, \dots, B\}$ }
			\STATE Realizar una permutación $\mathbf{X}^*$ de  $\mathbf{X}$.
			\STATE Calcular $T^* = T(\mathbf{X}^*)$
		\ENDFOR
		\STATE El conjunto $\{ \mathcal{X}^*_b, b = 1, \dots, B \}$ es una muestra aleatoria de $\Omega_{/\mathbf{X}}$ y los valores correspondientes $\{ T^*_b, b = 1, \dots, B \}$ simulan la distribución nula de las permutaciones de $T$ (aquella que tendría $T$ de ser cierta $H_0$). Entonces el $p$-valor se estima como $\hat{\lambda}(\mathbf{X}) = \sum\limits_{b=1}^B \mathbb{I}[T_b^* \geq T^0]/B$, esto es, la proporción de los valores de la permutación mayores que el observado.
	\end{algorithmic}
\end{algorithm}
	
\subsection{Revisión del estado del arte en la aplicación de test no paramétricos}
 
\paragraph{Validación de un modelo de degradación de documentos} \cite{KANUNGO00} En este artículo se presenta un modelo de la degradación en documentos producida por la digitalización de documentos impresos o la realización de fotocopias. El problema estadístico que se define es el siguiente. Dadas las muestras $x_1, \dots, x_N$ e $y_1, \dots, y_M$ de caracteres degradados de un mismo carácter de un documento y generados de manera artificial, respectivamente, realizar el test sobre la hipótesis nula de de ambos  provienen de la misma población para un valor de significación $\alpha$. Se aplica un test basado en permutaciones calculando el $p$-valor utilizando el algoritmo~\ref{alg:CMC-pvalue}.

\begin{algorithm}
	\caption{Test basado en permutaciones}
	\begin{algorithmic}[1]
	\REQUIRE
		\begin{enumerate}[a]
		\item Datos reales $X = \{ x_1, \dots, x_N \}$
		\item Datos generados $Y = \{ y_1, \dots, y_M \}$
		\item Función distancia entre conjuntos $\rho(X,Y)$
		\item Función distancia entre caracteres $\delta(x,y)$
		\item Tamaño máximo del test $\alpha$
		\end{enumerate}
		\STATE Calcular $d_0 = \rho(X,Y)$
		\STATE Crear una muestra $Z= \{x_1, \dots, x_N, y_1, \dots, y_M \}$.
		\FOR { $i \in \{1, \dots, K\}$ }
			\STATE Realizar una permutación de $Z$.
			\STATE Particionar $Z$ en $X'$ e $Y'$ tales que $X' = \{ z_1, \dots, z_N \}$, $Y' = \{ z_{N+1}, \dots, z_{N+M} \} $.
			\STATE Calcular $d_i = \rho(X,Y)$
		\ENDFOR
		\STATE Calcular la distribución empírica de las $d_i$
		\STATE Calcular el $p$-valor: $\alpha_0 = P(d \geq d_0)$
		\STATE Rechazar la hipótesis nula si $\alpha_0 \leq \alpha$
	\end{algorithmic}
\end{algorithm}

	Para la comparar un modelo $A$ con otro $B$ se comparan sus funciones de potencia. Suponemos $X \leadsto F(\theta_X)$, $Y \leadsto F(\theta_Y)$. Consideraremos la hipótesis nula $H_0 = \theta_X = \theta_Y$. Si fijamos $\theta_X= \theta_0$, Denotamos a la función potencia $\gamma_{\theta_0}= P(H_1 | \theta_X = \theta_0 \text{ y } \theta_Y = \theta)$. $A$ será mejor que $B$ dado $\theta$ si $\gamma_{\theta_0}^A  >\gamma_{\theta_0}^B$.
	
\paragraph{Comparación estadística de varios clasificadores} \cite{CHENGCHEN03} En este artículo se describe un método para realizar la comparación estadística de varios clasificadores sobre una misma base de datos a través del test de Cochran, una generalización del test de McNemar. La aplicación de un test no paramétrico en lugar de uno paramétrico como ANOVA se justifica en el artículo debido a que no se puede asegurar la independencia entre las instancias clasificadas por un mismo clasificador, con lo que queda en entredicho el desempeño del test ANOVA, por ejemplo. Si la hipótesis nula, $H_0: \theta_1 = \dots = \theta_n$ (todos los algoritmos tienen un rendimiento equivalente), es cierta, no es necesario ningún análisis posterior. Si es rechazada, se pretenden realizar múltiples comparaciones para encontrar los algoritmos con mejor rendimiento. Para ello se describen los métodos de Bonferroni y Scheffé. El primero parte de una familia de $g$ contrastes, mientras que el segundo considera la familia $L$ de todos los contrastes posibles. Por ello, para el primero se deben prefijar las comparaciones a realizar, mientras que para el segundo esto se puede realizar \textit{a posteriori} según los datos obtenidos. Para escoger entre un método u otro, se recomienda escoger el primero cuando $Z_{1-\alpha/2g}$ (percentil $(1-\alpha/2g)100$ de una distribución normal estándar) es menor que $\sqrt{\chi^2_{m-1,1-\alpha}}$ (donde $m$ es el número de algoritmos en la comparación) debido a que en este caso el método de Bonferroni tiene una mayor potencia. Escogeremos el método de Scheffé en caso contrario.

\paragraph{Comparación estadística de clasificadores en varios conjuntos de datos} \cite{DEMSAR06} De la comparación utilizando un único conjunto de datos se pasa en este artículo a presentar un método para comparar el rendimento sobre varios conjuntos de datos utilizando test no paramétricos.\\
	Se incluyen en una primera sección las opciones para realizar las comparaciones entre dos clasificadores para varias bases de datos, evaluando sus ventajas e inconvenientes:
	\begin{enumerate}
	\item Media entre los conjuntos de datos: Arroja poca información poco valiosa al combinar puntuaciones obtenidas en distintos dominios.
	\item $t$-test emparejados: El estadístico de este test paramétrico es similar al usado al realizar la media entre los conjuntos de datos. Además, se necesita que la diferencia siga una distribución normal, además de que los datos disponibles suelen ser reducidos. 
	\item Test de signo: Como se ha dicho en el desarrollo de los test, no tiene en cuenta la magnitud de la diferencia.
	\item \textit{Signed-Rank test} de Wilcoxon: Más seguro que el $t$-test al no suponer la distribución normal. Si no se dan las circunstancias adecuadas, este test puede tener mayor potencia que el $t$-test.
	\end{enumerate}
	
	En cuanto a la comparación entre varios clasificadores, se incluye una breve descripción del método ANOVA y una descripción más extensa del test de Friedman, que podríamos considerar su versión no paramétrica. Se incluye el estadístico $F_F = \frac{(n-1) \chi^2_F}{n(k-1) - \chi^2_F}$ que sigue la distribución $F$ con $k-1$ y $(k-1)(n-1)$ grados de libertad. Aunque el test ANOVA tiene mayor potencia cuando las condiciones se cumplen, se incluye un experimento que muestra que apenas hay diferencias si éstas no se dan.\\
	Una vez se ha rechazado la hipótesis nula se describen los test \textit{post-hoc} de Nemenyi, Bonferroni-Dunn, Holm, Hochberg y Hommel.
	
	
\paragraph{Extensión sobre los procedimientos \textit{post-hoc}} \cite{GARCIAHERRERA08} Este artículo extiende al anterior prestando una mayor atención a la descripción de los procedimientos para la comparación entre algoritmos una vez rechazada la hipótesis nula. 
	\begin{itemize}
	\item Procedimiento de Nemenyi: Se ajusta el valor de $\alpha$ dividiendo por el número de comparaciones $\frac{k(k-1)}{2}$. 
	\item Procedimiento de Holm: Ajusta el valor de $\alpha$ de manera descendente. Siendo $p_1, \dots, p_m$ los $p$-valores ordenados de menor a mayor y $H_1, \dots, H_m$ sus correspondientes hipótesis, se rechazan $H_1, \dots, H_{i-1}$ para $i$ el menor número tal que $p_i > \alpha/(m-i+1)$
	\end{itemize}
	
	Debido a que las posibles hipótesis sobre la comparación por parejas están relacionadas y no se pueden dar todas las posibles combinaciones, se presentan los procedimientos de Schaffer estático y dinámico. Bergmann y Hommel propusieron un procedimiento basado en la idea de encontrar todas las hipótesis que no pudieran ser rechazadas. En un estudio comparativo sobre el rendimiento de cinco clasificadores sobre treinta bases de datos se observa como el procedimiento de Bergmann y Hommel tiene una mayor potencia.\\
	Se aborda también el problema de calcular $p$-valores ajustados (APV) que tengan en cuenta los demás test realizados para poder utilizar directamente este valor ajustado. Se indican el cálculo para cada procedimiento \textit{post-hoc}. Tras realizar un nuevo estudio introduciendo el uso de APV, se descarta el test de Nemenyi debido a que es demasiado conservador. Se recomienda el procedimiento estático de Schaffer frente al del Holm al tener mayor potencia al usar las relaciones entre las hipótesis y una dificultad no muy superior. Debido a su coste computacional y su complejidad, se recomienda el método de Bergmann-Hommel sólo para cuando las diferencias entre clasificadores sean muy pequeñas y otros métodos no detecten diferencias significantes.

	\paragraph{Estudio del uso de test no paramétricos} \cite{GARCIA09} En este artículo se realiza un estudio sobre el uso de test no paramétricos para comparar el rendimiento de algoritmos evolutivos en la Sesión Especial de Optimización de parámetro real en el Congreso de Computación Evolutiva (CEC 2005). Los datos disponibles son los errores cometidos por los (11) algoritmos a la hora de minimizar 25 funciones de 10, 30 y 50 variables reales. En primer lugar se estudian las condiciones para realizar un test paramétrico ($t$-test) llevando a cabo test de normalidad (test de Kolmogoro-Smirnov, test de Shapiro-Wilk y el de D'Agostino-Pearson) y el test de Levene de Heterocedasticidad. Se pretende realizar en primer lugar la comparación entre dos de los algoritmos para comprobar si hay diferencias entre ellos. Sin embargo los test de normalidad rechazan para los resultados en casi todas las funciones a minimizar la hipótesis de normalidad y el test de Levene rechaza la hipótesis de la equivalencia de las varianzas, con lo que no se cumplen las condiciones para realizar el $t$-test. Si se realizan el $t$-test y el test no paramétrico de Wilcoxon se observa que los resultados son muy similares.\\
	A la hora de afrontar la comparación en múltiples problemas en lugar de problema por problema en este artículo se toma la media de las distintas ejecuciones. Al realizar los test de normalidad sobre las dos muestras de 25 elementos para cada algoritmo se descarta la normalidad. El $t$-test y el test de Wilcoxon no rechazan la hipótesis nula de que ambos algoritmos sean igual de buenos, sin embargo el $p$-valor del test de Wilcoxon es bastante menor, lo que nos lleva a pensar que, debido a la ausencia de normalidad y de pocos datos y difíciles de incrementar (pues en este caso deberíamos añadir funciones nuevas funciones y sus correspondientes evaluaciones para cada algoritmo), en estos casos es mejor utilizar test no paramétricos.\\
	Para realizar el análisis sobre todos los algoritmos se separan las funciones fáciles, aquellas en las que algún algoritmo alcanza el óptimo frente a las que ningún algoritmo llegó al óptimo. El test no paramétrico de Friedman y el de Iman-Davenport arrojan valores que nos llevan a rechazar la hipótesis nula tanto usando el grupo de funciones difíciles como usando el conjunto total de funciones.\\
	El estudio posterior se centra en comparar los algoritmos con aquel que consiguió un menor error medio, el \textit{G-CMA-ES}. Para ello se utilizan el test de Bonferroni-Dunn, el de Holm y el de Hochberg para los dos grupos de funciones. El test de Hochberg llega a encontrar diferencias entre el mejor algoritmo y todos los demás para $\alpha=0.1$. 
	
	\paragraph{Estudio estadístico sobre algoritmos genéticos: precisión e interpretabilidad} \cite{GARCIA08} En este artículo se realiza la comparación sobre algoritmos evolutivos utilizados en clasificación, comparando tanto la tasa de clasificación como el valor kappa de Cohen y la interpretabilidad de estos resultados. Al igual que en el artículo anterior, se realizan test de normalidad y heterocedasticidad para comprobar si se dan las condiciones para aplicar un test paramétrico. Si las comparaciones por parejas de problemas se repiten crece el error asociado FWER (\textit{family-wise error rate}), esto es la probabilidad de al menos un error en la familia de hipótesis. Para realizar comparaciones entre varios algoritmos y fijar el FWER de antemano se utilizan el test de Friedman y el de Iman-Davenport y los test de Bonferroni-Dunn y el de Holm para buscar diferencias entre los algoritmos una vez descartada la hipótesis de que todos tienen la misma media. Se realiza calculan también los $p$-valores ajustados.\\
	Para realizar el análisis de la interpretabilidad de los resultados se considera $\textit{Complejidad} = \textit{Tamaño} \cdot ANT$. Se realiza para este estudio únicamente un procedimiento para múltiples comparaciones, obteniendo diferencias significativas con el test de Bonferroni-Dunn y nivel de significación $\alpha=0.1$.
	
	\paragraph{Análisis \textit{bootstrap} de múltiples repeticiones de experimentos} \cite{OTERO13} En este artículo se presenta la realización de test basados en permutaciones para la comparación del rendimiento de algoritmos de clasificación, realizando un estudio para el que el test presentado tiene mayor potencia que los test de ANOVA o Friedman. El contexto para el que se presenta este método es el de la comparación entre algoritmos estocásticos (de ahí que se realice la repetición de la ejecución de los algoritmos con diferentes semillas) utilizando validación cruzada. Se propone usar intervalos para recoger parte de la variabilidad de los datos atribuible a la repetición del experimento. Se plantea el problema del uso de la media entre los distintos experimentos para evaluar un algoritmo debido a que los valores extremos tienen una influencia excesiva y se aboga por otros estadísticos como la mediana, el rango intercuartílico o la media restringida a los valores centrales.\\
	El primer test es un test basado en permutaciones. Parte de la muestra compuesta por $e_{adfr}$, que es el error del algoritmo $a$-ésimo, el conjunto de datos $d$, para el \textit{fold} $f$ y la repetición $r$. Se utiliza en este test como estadístico para evaluar cada algoritmo la media de las ejecuciones en todos los \textit{folds}, repeticiones y bases de datos como estimador, utilizando el algoritmo ~\ref{alg:CMC-pvalue}. Se rechazará la hipótesis nula (la equivalencia de las medias) si para algún $a$ la media inicial $\hat{e}_a$ se encuentra en las colas de la distribución formada por los valores de las permutaciones $\hat{e}_a^*$ consistente en la cdf muestral:
	\[ 
		\hat{F}_{a \cdot \cdot \cdot}(x) =
			\frac{1}{n_d n_r n_f}
			\sum\limits_{i=1}^{n_d}
				\sum\limits_{j=1}^{n_f}
					\sum\limits_{k=1}^{n_r}
						\mathbb{I}[ e_{aijk} \leq x ].
	\]
	El segundo test sí hace uso de los intervalos, notando por $[q_{-adf}, q_{+adf}]$ el intervalo de los errores para un algoritmo, base de datos y \textit{fold}. Se definen las cdf muestrales como
	
\begin{align*}
	\hat{F}_{-a \cdot \cdot}(x) &=
		\frac{1}{n_d n_f}
			\sum\limits_{i=1}^{n_d}
				\sum\limits_{j=1}^{n_f}
					\mathbb{I}[ q_{+aij} \leq x ], \\
	\hat{F}_{+a \cdot \cdot}(x) &=
		\frac{1}{n_d n_f}
			\sum\limits_{i=1}^{n_d}
				\sum\limits_{j=1}^{n_f}
					\mathbb{I}[ x \in [q_{-aij},q_{+aij}) ] +
			\hat{F}_{-a \cdot \cdot}(x) .			
\end{align*}

	En este test se aplica también el algoritmo ~\ref{alg:CMC-pvalue}, rechazando la hipótesis nula si el intervalo $[\hat{q}_{-a}, \hat{q}_{+a}]$, con $\hat{q}_{-a}, \hat{q}_{+a}$ las medias de los extremos inferiores y superiores respectivamente para cada algoritmo con respecto a las bases de datos y \textit{fold}, se encuentra en las colas de la distribución de los $[\hat{q}_{-a}^*, \hat{q}_{+a}^*]$.\\
	Los resultados incluidos ofrecen una comparación entre la potencia de estos test presentados, el test de Friedman y el de ANOVA. Se realiza un experimento con datos sintéticos consistentes en los resultados de los errores de 5 algoritmos en 32 bases de datos para una secuencia de diferencias en sus medias preestablecidas. Para calcular la potencia se estima para cada diferencia el porcentaje de experimentos para el que se encuentran diferencias significativas entre los algoritmos dos a dos. Los resultados indican una mayor potencia de los test presentados en este artículo.


\section{Test bayesianos}	
