%\documentclass[11pt,leqno]{book}
%\usepackage[spanish,activeacute]{babel}
%\usepackage[utf8]{inputenc}
%\usepackage{enumerate}
%
%\begin{document}

\chapter{Desarrollo informático}

\section{Aplicación de test estadísticos en el aprendizaje automático}

\subsection{Introducción}

	A la hora de evaluar un algoritmo de aprendizaje automático nos interesa conocer el error para un problema y su relación con respecto a otros, sin embargo, debemos ser conscientes de que para ello debemos diseñar y realizar un experimento cuyos resultados sean fiables y minimizar el efecto producido por las circunstancias en las que se realizan o los datos disponibles. Por ejemplo, el error obtenido en los datos utilizados para entrenar el modelo (error de entrenamiento) será menor que el error en unos datos que no han participado en este entrenamiento y son usados para evaluar el modelo (error de test). Cabe mencionar también que el error de entrenamiento no es comparable debido a que modelos más complejos se ajustarán más a los datos de entrenamiento, sin que eso signifique que el modelo se comporte mejor de manera global. Hay algoritmos que se comportan de manera no determinista, y por tanto deseemos realizar varias ejecuciones para estimar circunstancias aleatorias. En definitiva, debemos basar nuestra evaluación del algoritmo en la distribución (desconocida) del error. Para una correcta estimación del error es importante tener en cuenta lo siguiente:
	
	\begin{itemize}
	\item El error depende del problema. El teorema de \textit{No Free Lunch} afirma que dados dos algoritmos su rendimiento medio para todos los posibles problemas es el mismo, lo que se traduce en que según el problema se puede dar que un algoritmo sea mejor que otro o al contrario.
	\item La partición de la base de datos referente al problema en datos de entrenamiento y de test se realiza para obtener una idea más acertada del error cometido por el algoritmo para el problema. Sin embargo, una vez que se considere válido el algoritmo se entrenará con todos los datos disponibles.
	\item Para validar el algoritmo, se deben utilizar datos que no hayan sido utilizados anteriormente de ninguna manera.
	\item Normalmente, para la comparación del rendimiento de algoritmos se usa el error cometido en la clasificación, aunque se podrían considerar otros factores como el tiempo necesario para entrenar el algoritmo, su simplicidad...
	\end{itemize}
	
\subsubsection{Diseño de experimentos}

	Al hablar de experimentos nos referimos al test o a la serie de test donde se manejan factores que afectan a la salida. Pretendemos minimizar aquellos factores que no se controlan y obtener resultados estadísticamente significativos. Para ajustar parámetros propios del modelo de aprendizaje automático que estemos utilizando existen diferentes aproximaciones: la \textit{mejor suposición} (donde quien realiza el experimento ajusta los parámetros basándose en su conocimiento sobre el problema), \textit{un factor cada vez} (se supone que los factores son independientes y se prueban diferentes valores para cada parámetro de uno en uno, mientras los demás se mantienen en una posición base) y \textit{diseño factorial} (se crea una rejilla con diferentes posibles valores para cada parámetro y se prueba en ellos, es más costoso).\\
	Para minimizar el número de ejecuciones necesarias, se suele utilizar conocimiento previo para reducir las combinaciones a probar. Otra estrategia consiste en el diseño de la superficie de respuesta. Se considera la siguiente situación:
		\[ r = g( f_1, \dots, f_F | \phi ), \]
	donde $r$ es la respuesta, $g$ el modelo utilizado, $f_1, \dots f_F$ los factores y $\phi$ la estimación empírica del modelo para cada configuración particular probada. El procedimiento consiste en ir añadiendo a $\phi$ las evaluaciones realizadas, ajustar $g$ según $\phi$ y buscar su máximo (o mínimo), evaluar el algoritmo en ese punto y ajustar $\phi$.\\
	En la realización de experimentos es necesaria la aleatorización a la hora de probar las diferentes configuraciones de parámetros (no tanto en nuestro contexto, pero el orden de las configuraciones al realizar experimentos donde interviene maquinaria por ejemplo podría inferir en los resultados debido a factores como la temperatura de la máquina). Otro factor a tener en cuenta es que es necesaria la repetición de las ejecuciones y promediar los resultados para así reducir el impacto de factores aleatorios. Para reducir la variabilidad de factores que no dependen del propio algoritmo a evaluar, como las bases de datos o las particiones realizadas, en la comparación de varios algoritmos utilizamos las mismas particiones para cada uno.

\paragraph{Directrices a la hora de realizar un experimento}
	\begin{enumerate}
	\item Fijar la intención del estudio
	\item Seleccionar las variables de respuesta
	\item Seleccionar los factores con los que se realizará el experimento y los niveles a comprobar
	\item Diseño del experimento: La partición para entrenamiento y test depende del tamaño de la base de datos. Si la base de datos es pequeña puede haber una alta variabilidad y obtener resultados no concluyentes.
	\item Efectuar el experimento: Antes de llevar a cabo un experimento de gran magnitud es aconsejable probar pequeños conjuntos de datos para probar que funciona. También es interesante guardar resultados intermedios o las semillas aleatorias para facilitar la reproducción de los datos.
	\item El experimentador debe ser imparcial y juzgar de igual manera un algoritmo que otro, realizar documentación ...
	\item Análisis estadístico: las afirmaciones y preguntas que se realicen debe sostenerse estadísticamente hablando. Es aconsejable un análisis visual para exponer los datos obtenidos.
	\end{enumerate}
	
	
\subsubsection{Validación cruzada y remuestreo}
	
	Para la repetición de los experimentos, necesitamos un número de conjuntos de entrenamiento y validación del conjunto de datos disponible. Si el conjunto es suficientemente grande, podemos dividir aleatoriamente el conjunto en $K$ partes, y de cada parte seleccionar la mitad para entrenamiento y la otra mitad para test. $K$ suele ser 10 ó 30. Sin embargo, los conjuntos de datos no suelen ser tan grandes. Por tanto, lo que habitualmente se realiza es utilizar varias veces los mismos datos pero de formas distintas, lo que se conoce como \textbf{validación cruzada} (\textit{cross-validation}, CV). Otro concepto a tener en cuenta es el de \textbf{estratificación}. Consiste en mantener la proporción de las diferentes clases en cada una de las particiones realizadas en la base de datos. 
	
\paragraph{Validación cruzada con $K$-fold} Se divide el conjunto de datos $\mathcal{X}$ en $K$ partes aleatoriamente, $\mathcal{X} = \mathcal{X}_1 \cup \dots \cup \mathcal{X}_K$. Para formar cada par de datos de entrenamiento y validación, $(\mathcal{T}, \mathcal{V})$, se mantiene una de las $K$ partes y se unen las demás partes como conjunto de entrenamiento
\begin{align*}
	\mathcal{V}_i &= \mathcal{X}_i \\
	\mathcal{T}_i &= \underset{j=1,\dots,K; j\neq i}
							\bigcup \mathcal{X}_j
\end{align*} 
	El problema de este método es que el conjunto de validación es pequeño (lo que lleva a una mayor variabilidad en el error). Además, los conjuntos de entrenamiento se solapan considerablemente. $K$ es habitualmente 10 ó 30. Conforme $K$ crece, se incrementa la robustez al afectar menos cada dato concreto. El caso $K = n-1$, se conoce como \textit{leave-one-out}. 

\paragraph{$5 \times 2$ CV} Otra propuesta consiste en dividir $\mathcal{X}$ en dos partes, utilizar una de estas partes como datos de entrenamiento y la otra como test e invertir los roles. Para obtener el segundo \textit{fold}, se realiza otra partición aleatoria de $\mathcal{X}$ y se repite el proceso. Lo habitual es que se realice cinco veces, obteniendo así diez parejas $(\mathcal{T}, \mathcal{V})$. Aunque podría realizarse más veces, al realizarse más veces los conjuntos comparten muchas instancias y por lo tanto los errores son muy dependientes entre sí, con lo que no se aporta nueva información. Si se realiza menos de cinco veces, se obtienen pocas muestras y es más complejo comprobar las hipótesis.

\paragraph{\textit{Bootstrapping}} Para generar varias muestras a partir de una única se seleccionan instancias de la original con reemplazamiento. Puede solaparse más que con CV, con lo que las estimaciones resultan más dependientes. Por ello, se recomienda especialmente para conjuntos de datos muy pequeños en los que se necesiten mayores muestras. 
	Al realizar \textit{bootstrap}, se seleccionan $N$ instancias de un conjunto de tamaño $N$ con reemplazamiento. El conjunto original se usa como conjunto de validación. La probabilidad de no seleccionar una instancia es $1-1/N$, con lo que la probabilidad de no seleccionarla tras $N$ elecciones es $\left(1- \frac{1}{N}\right)^N \approx e^{-1} \approx 0.368$. Esto significa que el conjunto de entrenamiento contiene aproximadamente un $63.2\%$ de los datos. Para obtener una mejor estimación del error, se propone repetir el proceso y observar el comportamiento medio.
	
\subsubsection{Test de hipótesis}

	En la realización de los test de hipótesis, el procedimiento es el siguiente: se define un estadístico que sigue una distribución conocida en el caso de que se cumpla la hipótesis realizada. Entonces si el estadístico calculado de la muestra tiene poca probabilidad de obtenerse de la distribución, se rechaza la hipótesis; en caso contrario, no. \\
	Surgen entonces las cuestiones: ¿Se puede justificar los resultados estadísticamente o se obtienen por azar? ¿Son los conjuntos de datos representativos para el problema? Estas preguntas no se pueden responder de forma exhaustiva, es la labor de los test estadísticos reunir los datos disponibles para justificar la consistencia de las conclusiones. \\
	Hay objeciones la los test de hipótesis nula. En primer lugar, estos test se pueden concebir como híbrido de los propuestos por Fisher por un lado y por Neyman-Pearson por otro. Además, hay malentendidos frecuentes a la hora de obtener conclusiones y no nos dicen todo lo que necesitamos conocer para la comparación de los algoritmos: Si realizamos un experimento con $\alpha=0.01$ y descartamos la hipótesis nula de que $f_A$ y $f_B$ tengan igual rendimiento, la conclusión no es que $f_A$ sea mejor que $f_B$ con una probabilidad del $99\%$ sino que ésta es la probabilidad de que los datos sean correctos si $f_A$ es mejor que $f_B$. Otro interpretación errónea común es suponer que $1-\alpha$ es la probabilidad de obtener esos mismos resultados de nuevo.\\
	Sin embargo los test de hipótesis siguen siendo una herramienta muy útil si se aplica correctamente y entendiendo las implicaciones, además de que las alternativas presentadas no han resultado ser tan utilizadas ni son ajenas a las críticas.\\
	A la hora de realizar los test estadísticos podemos realizar las siguientes comparaciones:
	\begin{enumerate}
	\item Comparación de dos algoritmos en un dominio: $t$-test (paramétrico), test de McNemar (no paramétrico).
	\item Comparación de múltiples algoritmos en un dominio: \textit{Signed-Rank test} de Wilcoxon y test de signo (no paramétricos).
	\item Comparación de múltiples algoritmos en varios dominios: ANOVA (paramétrico), test de Friedman o basados en permutaciones (no paramétricos). 
	\end{enumerate}

	Nótese que el test de \textit{Signed-Rank} de Wilcoxon para muestras emparejadas puede aplicarse tanto para la comparación de dos como entre varios algoritmos en un dominio. Los test para varios algoritmos en varias bases de datos necesitan un test posterior en caso de rechazar la hipótesis de que todos los algoritmos se comporten de igual manera. Ejemplos de estos test son el de Tukey, de Bonferroni-Dunn o de Nemenyi. 
	
	
	
	
	
	
	
	
%\end{document}
