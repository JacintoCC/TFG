\part{Desarrollo informático}
\label{part:informatica}

\chapter{Revisión del estado del arte}
\label{chapter:revision}

\section{Test paramétricos}

\paragraph{Problemas a evitar y enfoque recomendado} 
\cite{Salzberg1997} Este es uno de los primeros artículos 
que hace una serie de recomendaciones para la realización
de experimentos en el ámbito de la comparación 
de algoritmos de aprendizaje automático. Se incluyen 
también varias consideraciones sobre los problemas y
prácticas incorrectas al realizar estos experimentos. Los
test propuestos para la comparación son el test de McNemar, 
el $t$-test y el test ANOVA para múltiples algoritmos. Para
la realización de varios experimentos independientes 
se propone el ajuste de Bonferroni del nivel de significación
$\alpha$, que necesita que los experimentos sean
independientes.


\subsection*{5$\times$2 CV $t$-test}
	
	En la introducción (\ref{sec:CV}), se describe el 
proceso de validación cruzada, especialmente útil cuando se
disponen de pocos datos y queremos mantener un equilibrio
entre el tamaño del conjunto de entrenamiento y el de test.
Para esta situación, se propone la siguiente variación 
del $t$-test. Se considera $d_i^{(j)}$, para $j=1,2$, $i=
1,\dots,5$, la diferencia entre el rendimiento de los dos
clasificadores para la repetición $i$ y el \textit{fold} $j$.
Se considera la media para cada repetición $\bar{d}_i = 
(d_i^{(1)} + d_i^{(2)})/2$ y la varianza estimada
$s_i^2 = (d_i^{(1)}-\bar{d_i})^2 + (d_i^{(2)}-\bar{d_i})^2$. 
Bajo la hipótesis nula de que los dos clasificadores tienen 
el mismo rendimiento, $d_i^{(j)}$ puede considerarse 
aproximadamente normal . Si suponemos $d_i^{(1)}$ y 
$d_i^{(2)}$ independientes y normales (lo cual no es
cierto porque los conjuntos de test y de entrenamiento
no se han obtenido de manera independiente), entonces
$s_i^2/\sigma^2$ sigue una distribución $\chi^2$ con un
grado de libertad. Si suponemos las $s_i^2$ independientes,
su suma $M = \frac{\sum\limits_{i=1}^5 s_i^2}{\sigma^2}
\sim \chi^2_5$ y 
\begin{equation}
	\label{eq:5x2CVt}
	t = \frac{d_1^{(1)}/\sigma}{\sqrt{M/5}} = 
		\frac{d_1^{(1)}}
			{\sqrt{\sum\limits_{i=1}^5 s_i^2/5}}
		\sim t_5.
\end{equation}


\paragraph{Comparación de algoritmos de clasificación} 
\cite{dietterich1998approximate}
En este artículo se estudia la comparación de algoritmos
aplicados a un único problema cuando disponemos de pocos
datos. Para el diseño y la evaluación de test estadísticos
se identifican las principales fuentes de 
variabilidad de los experimentos, como son los 
datos utilizados, la posible aleatoriedad de los 
propios algoritmos de aprendizaje y los posibles errores en
los datos disponibles. Se proponen los test de McNemar, un
test basado en él, usando el estadístico
	\[
		z = \frac{c_{01}-c_{10}}{\sqrt{2p(1-p)}}
	\]
	con $p$ el error medio de los dos clasificadores $\left(
p = \frac{2c_{00}+c_{01}+c_{10}}{2N}\right)$, que sigue
aproximadamente una distribución normal. Además,
se proponen las modificaciones del $t$-test con repetidas
muestras apareadas, el $t$-test pareado con validación
cruzada sobre$k$-\textit{fold} o el 5$\times$2 CV $t$-test.\\
	El experimento realizado consiste en, fijado un error 
$\varepsilon$, se le asigna a la mitad de los puntos
de un conjunto una probabilidad $\frac{\varepsilon}{2}$
de ser erróneamente clasificados
y a la otra mitad de los puntos la probabilidad
$\frac{3\varepsilon}{2}$ para uno de los algoritmos a
comparar. Para el otro algoritmo asignamos a la primera
mitad la probabilidad $\frac{3\varepsilon}{2}$ y a la
segunda mitad la probabilidad de fallar 
$\frac{\varepsilon}{2}$. Por tanto no deberíamos
rechazar la hipótesis nula, con lo que se realizan estos
experimentos para calcular el error de tipo I
para cada test. Se observa con la utilización de estos
datos generados sintéticamente que el error de tipo I más
elevado es para el $t$-test con repetidas
muestras apareadas.


\paragraph{Modificación sobre el 5$\times$2 CV $t$-test} Nótese que en la 
ecuación~\ref{eq:5x2CVt} el valor $d_1^{(1)}$ podría ser
ocupado por cualquier estadístico $d_i^{(j)}$. Alpaydin 
(\cite{Alpaydin98combined5})
propuso combinar estos valores de la forma:
	\[
		N = \frac{\sum\limits_{i=1}^5
				\sum\limits_{j=1}^2
					\left( d_i^{(j)} \right)^2}
				{\sigma^2}
			\sim \chi_{10}^2.
	\]
	Sustituyendo esto en \ref{eq:5x2CVt}, tenemos
un estadístico que es el cociente de dos v.a. que siguen 
una $\chi^2$, con lo que definimos la v.a. $f$, que seguirá
una distribución $F$ de Snedecor con 10 y 5 grados de
libertad:
	\[ 
		f = \frac{N/10}{M/5} \sim F_{10,5}.
	\]	
	
\section{Test no paramétricos}
 
\paragraph{Validación de un modelo de degradación de 
documentos} \cite{DBLP:journals/pami/KanungoHBSM00} En este artículo se presenta un 
modelo de la degradación en documentos producida por la 
digitalización de documentos impresos o la realización de 
fotocopias. El problema estadístico que se define es el 
siguiente. Dadas las muestras $x_1, \dots, x_N$ e $y_1, 
\dots, y_M$ de caracteres degradados de un mismo carácter de 
un documento y generados de manera artificial, 
respectivamente, realizar el test sobre la hipótesis nula de 
que ambos  provienen de la misma población para un valor de 
significación $\alpha$. Se aplica el algoritmo 
(\ref{alg:CMC-digitalizacion}) para realizar el test basado 
en permutaciones calculando el $p$-valor utilizando el 
algoritmo~\ref{alg:CMC-pvalue}.

\begin{algorithm}
	\caption{Test basado en permutaciones para validación en 
			modelo de degradación}
	\label{alg:CMC-digitalizacion}
	\begin{algorithmic}[1]
	\REQUIRE
		\begin{enumerate}[a]
		\item Datos reales $X = \{ x_1, \dots, x_N \}$
		\item Datos generados $Y = \{ y_1, \dots, y_M \}$
		\item Función distancia entre conjuntos $\rho(X,Y)$
		\item Función distancia entre caracteres $\delta(x,y)$
		\item Tamaño máximo del test $\alpha$
		\end{enumerate}
		\STATE Calcular $d_0 = \rho(X,Y)$
		\STATE Crear una muestra 
			$Z= \{x_1, \dots, x_N, y_1, \dots, y_M \}$.
		\FOR { $i \in \{1, \dots, K\}$ }
			\STATE Realizar una permutación de $Z$.
			\STATE Particionar $Z$ en $X'$ e $Y'$ tales que 
				$X' = \{ z_1, \dots, z_N \}$, $Y' = 
				\{ z_{N+1}, \dots, z_{N+M} \} $.
			\STATE Calcular $d_i = \rho(X,Y)$
		\ENDFOR
		\STATE Calcular la distribución empírica de las $d_i$
		\STATE Calcular el $p$-valor: 
			$\alpha_0 = P(d \geq d_0)$
		\STATE Rechazar la hipótesis nula si 
			$\alpha_0 \leq \alpha$
	\end{algorithmic}
\end{algorithm}

	
\paragraph{Comparación estadística de varios clasificadores} 
\cite{DBLP:conf/mlmta/ChenC03} En este artículo se describe un método 
para realizar la comparación estadística de varios 
clasificadores sobre una misma base de datos a través del 
test de Cochran, una generalización del test de McNemar. La 
aplicación de un test no paramétrico en lugar de uno 
paramétrico como ANOVA se justifica en el artículo debido a 
que no se puede asegurar la independencia entre las 
instancias clasificadas por un mismo clasificador, con lo que 
queda en entredicho el desempeño del test ANOVA, por ejemplo. 
Si la hipótesis nula, $H_0: \theta_1 = \dots = \theta_n$ 
(todos los algoritmos tienen un rendimiento equivalente), es 
cierta, no es necesario ningún análisis posterior. Si es 
rechazada, se pretenden realizar múltiples comparaciones para 
encontrar los algoritmos con mejor rendimiento. Para ello se 
describen los métodos de Bonferroni y Scheffé. El primero 
parte de una familia de $g$ contrastes, mientras que el 
segundo considera la familia $L$ de todos los contrastes 
posibles. Por ello, para el primero se deben prefijar las 
comparaciones a realizar, mientras que para el segundo esto 
se puede realizar \textit{a posteriori} según los datos 
obtenidos. Para escoger entre un método u otro, se recomienda 
escoger el primero cuando $Z_{1-\alpha/2g}$ (percentil $(1-
\alpha/2g)100$ de una distribución normal estándar) es menor 
que $\sqrt{\chi^2_{m-1,1-\alpha}}$ (donde $m$ es el número de 
algoritmos en la comparación) debido a que en este caso el 
método de Bonferroni tiene una mayor potencia. Escogeremos el 
método de Scheffé en caso contrario.

\paragraph{Comparación estadística de clasificadores en 
varios conjuntos de datos} \cite{DBLP:journals/jmlr/Demsar06} De la comparación 
utilizando un único conjunto de datos se pasa en este 
artículo a presentar un método para comparar el rendimento 
sobre varios conjuntos de datos utilizando test no 
paramétricos.\\
	Se incluyen en una primera sección las opciones para 
realizar las comparaciones entre dos clasificadores para 
varias conjuntos de datos, evaluando sus ventajas e 
inconvenientes:
	\begin{enumerate}
	\item Media entre los conjuntos de datos: Arroja poca 
		información poco valiosa al combinar puntuaciones 
		obtenidas en distintos dominios.
	\item $t$-test emparejados: El estadístico de este test 
		paramétrico es similar al usado al realizar la media 
		entre los conjuntos de datos. Además, se necesita que 
		la diferencia siga una distribución normal, además de 
		que los datos disponibles suelen ser reducidos. 
	\item Test de signo: Como se ha dicho en el desarrollo de 
		los test, no tiene en cuenta la magnitud de la 
		diferencia.
	\item Test de rangos con signo de Wilcoxon: Más seguro 
		que el $t$-test al no suponer la distribución normal. 
		Si no se dan las circunstancias adecuadas, este test 
		puede tener mayor potencia que el $t$-test.
	\end{enumerate}
	
	En cuanto a la comparación entre varios clasificadores, 
se incluye una breve descripción del método ANOVA y una 
descripción más extensa del test de Friedman. Se incluye el 
estadístico $F_F = \frac{(n-1) \chi^2_F}{n(k-1) - \chi^2_F}$ 
que sigue la distribución $F$ con $k-1$ y $(k-1)(n-1)$ grados 
de libertad. Aunque el test ANOVA tiene mayor potencia cuando 
las condiciones se cumplen, se incluye un experimento que 
muestra que apenas hay diferencias si éstas no se dan.\\

	Una vez se ha rechazado la hipótesis nula se describen 
los test \textit{post-hoc} de Nemenyi, Bonferroni-Dunn, Holm, 
Hochberg y Hommel.
	
	
\paragraph{Extensión sobre los procedimientos 
\textit{post-hoc}} \cite{garcia2008extension} Este artículo 
extiende al anterior prestando una mayor atención a la 
descripción de los procedimientos para la comparación entre 
algoritmos una vez rechazada la hipótesis nula. Se introducen
los procedimientos de Nemenyi y Holm. Debido a que las 
posibles hipótesis sobre la comparación  por parejas están 
relacionadas y no se pueden dar todas las posibles 
combinaciones, se presentan los procedimientos de 
Schaffer estático y dinámico. Bergmann y Hommel propusieron 
un procedimiento basado en la idea de encontrar todas las 
hipótesis que no pudieran ser rechazadas. En un estudio 
comparativo sobre el rendimiento de cinco clasificadores 
sobre treinta conjuntos de datos se observa como el 
procedimiento de Bergmann y Hommel tiene una mayor 
potencia.\\

	Se aborda también el problema de calcular $p$-valores 
ajustados (APV) que tengan en cuenta los demás test 
realizados para poder utilizar directamente este valor 
ajustado. Se indican el cálculo para cada procedimiento 
\textit{post-hoc}. Tras realizar un nuevo estudio 
introduciendo el uso de APV, se descarta el test de Nemenyi 
debido a que es demasiado conservador. Se recomienda el 
procedimiento estático de Schaffer frente al del Holm al 
tener mayor potencia al usar las relaciones entre las 
hipótesis y una dificultad no muy superior. Debido a su coste 
computacional y su complejidad, se recomienda el método de 
Bergmann-Hommel sólo para cuando las diferencias entre 
clasificadores sean muy pequeñas y otros métodos no detecten 
diferencias significantivas.

	\paragraph{Estudio del uso de test no paramétricos} 
	\cite{DBLP:journals/heuristics/GarciaMLH09} En este artículo se realiza un estudio
sobre el uso de test no paramétricos para comparar el 
rendimiento de algoritmos evolutivos en la Sesión Especial de 
Optimización de parámetro real en el Congreso de Computación 
Evolutiva (CEC 2005). Los datos disponibles son los errores 
cometidos por los algoritmos a la hora de minimizar 25 
funciones de 10, 30 y 50 variables reales. En primer lugar se 
estudian las condiciones para realizar un test paramétrico 
($t$-test) llevando a cabo test de normalidad (test de 
Kolmogoro-Smirnov, test de Shapiro-Wilk y el de D'Agostino-
Pearson) y el test de Levene de homocedasticidad. Se 
pretende realizar en primer lugar la comparación entre dos de 
los algoritmos para comprobar si hay diferencias entre ellos. 
Sin embargo los test de normalidad rechazan para los 
resultados en casi todas las funciones a minimizar la 
hipótesis de normalidad y el test de Levene rechaza la 
hipótesis de la equivalencia de las varianzas, con lo que no 
se cumplen las condiciones para realizar el $t$-test. Si se 
realizan el $t$-test y el test no paramétrico de Wilcoxon se 
observa que los resultados son muy similares.\\
	A la hora de afrontar la comparación en múltiples 
problemas en lugar de problema por problema en este artículo 
se toma la media de las distintas ejecuciones. Al realizar 
los test de normalidad sobre las dos muestras de 25 elementos 
para cada algoritmo se descarta la normalidad. El $t$-test y 
el test de Wilcoxon no rechazan la hipótesis nula de que 
ambos algoritmos sean igual de buenos, sin embargo el $p$-
valor del test de Wilcoxon es bastante menor, lo que nos 
lleva a pensar que, debido a la ausencia de normalidad y de 
pocos datos y difíciles de incrementar (pues en este caso 
deberíamos añadir nuevas funciones y sus 
correspondientes evaluaciones para cada algoritmo), en estos 
casos es mejor utilizar test no paramétricos.\\
	Para realizar el análisis sobre todos los algoritmos se 
separan las funciones fáciles, aquellas en las que algún 
algoritmo alcanza el óptimo frente a las que ningún algoritmo 
llegó al óptimo. El test no paramétrico de Friedman y el de 
Iman-Davenport arrojan valores que nos llevan a rechazar la 
hipótesis nula tanto usando el grupo de funciones difíciles 
como usando el conjunto total de funciones.\\
	El estudio posterior se centra en comparar los algoritmos 
con aquel que consiguió un menor error medio, el 
\textit{G-CMA-ES}. Para ello se utilizan el test de 
Bonferroni-Dunn, el de Holm y el de Hochberg para los dos 
grupos de funciones. El test de Hochberg llega a encontrar 
diferencias entre el mejor algoritmo y todos los demás para 
$\alpha=0.1$. 
	
	\paragraph{Estudio estadístico sobre algoritmos 
genéticos: precisión e interpretabilidad} \cite{Garcia2008} En 
este artículo se realiza la comparación sobre algoritmos 
evolutivos utilizados en clasificación, comparando tanto la 
tasa de clasificación como el valor kappa de Cohen y la 
interpretabilidad de estos resultados. Al igual que en el 
artículo anterior, se realizan test de normalidad y 
homocedasticidad para comprobar si se dan las condiciones 
para aplicar un test paramétrico. Si las comparaciones por 
parejas de problemas se repiten crece el error asociado FWER 
(\textit{family-wise error rate}), esto es la probabilidad de 
cometer al menos un error en la familia de hipótesis. Para realizar 
comparaciones entre varios algoritmos y fijar el FWER de 
antemano se utilizan el test de Friedman y el de Iman-
Davenport y los test de Bonferroni-Dunn y el de Holm para 
buscar diferencias entre los algoritmos una vez descartada la 
hipótesis de que todos tienen la misma media. Se 
calculan también los $p$-valores ajustados.\\
	Para realizar el análisis de la interpretabilidad de los 
resultados se considera $\textit{Complejidad} = 
\textit{Tamaño} \cdot ANT$. Se realiza para este estudio 
únicamente un procedimiento para múltiples comparaciones, 
obteniendo diferencias significativas con el test de 
Bonferroni-Dunn y nivel de significación $\alpha=0.1$.
	
	\paragraph{Test avanzados para la comparación y análisis 
experimental de la potencia} \cite{DBLP:journals/isci/GarciaFLH10} En este
artículo se introducen con respecto a otros artículos 
previos los test de múltiples signos y la estimación del 
contraste basado en medianas como test básicos para realizar
entre múltiples clasificadores y conjuntos de datos como 
test básicos, y los más avanzados del test de Friedman
de rangos alineados (\textit{Friedman Aligned Ranks}, 
Friedman-AR) y el test de Quade. Además, para realizar las 
comparaciones  posteriores  una vez descartada la hipótesis 
de un igual  rendimiento entre todos los algoritmos, se 
incluyen a los  procedimientos \textit{post-hoc} los de 
Holland y Finner  dentro de  los métodos descendentes, el de 
Rom como método  ascendente y el procedimiento de Li, para 
rechazar hipótesis en dos pasos.\\
	Se realiza un estudio sobre la potencia de los test
de Friedman, de Friedman-AR y de Quade, utilizando 8 y 4
algoritmos sobre 10 y 6 conjuntos de datos respectivamente. 
Se observa que el test de Friedman-AR es más conservador
cuando el número de algoritmos es elevado. En este estudio
se observa que el test de Quade se comporta mejor, sin
embargo depende de los conjuntos de datos utilizados. Para
el estudio sobre los test \textit{post-hoc}, se utilizan
8 algoritmos sobre 10 conjuntos de datos seleccionados
de forma pseudo-aleatoria (regulando mediante un parámetro la 
probabilidad de seleccionar una base de datos según la 
diferencia de los algoritmos a comparar en ella). Los 
resultados obtenidos hacen desaconsejar el test de 
Bonferroni-Dunn, en un siguiente grupo estarían Holm, 
Hochberg, Hommel, Holland y Rom con una potencia similar
y los que mejores resultados obtienen son el test de Finner
y el test de Li, el cual ofrece mejores resultados 
cuando las diferencias entre el algoritmo de control y los 
demás sean grandes.

	
	\paragraph{Análisis \textit{bootstrap} de múltiples 
repeticiones de experimentos} \cite{Otero201488} En este artículo 
se presenta la realización de test basados en permutaciones 
para la comparación del rendimiento de algoritmos de 
clasificación, realizando un estudio para el que el test 
presentado tiene mayor potencia que los test de ANOVA o 
Friedman. El contexto para el que se presenta este método es 
el de la comparación entre algoritmos estocásticos (de ahí 
que se realice la repetición de la ejecución de los 
algoritmos con diferentes semillas) utilizando validación 
cruzada. Se propone usar intervalos para recoger parte de la 
variabilidad de los datos atribuible a la repetición del 
experimento. Se plantea el problema del uso de la media entre 
los distintos experimentos para evaluar un algoritmo debido a 
que los valores extremos tienen una influencia excesiva y se 
aboga por otros estadísticos como la mediana, el rango 
intercuartílico o la media restringida a los valores 
centrales.\\
	El primer test es un test basado en permutaciones. Parte 
de la muestra compuesta por $e_{adfr}$, que es el error del 
algoritmo $a$-ésimo, el conjunto de datos $d$, para el 
\textit{fold} $f$ y la repetición $r$. Se utiliza en este 
test como estadístico para evaluar cada algoritmo la media de 
las ejecuciones en todos los \textit{folds}, repeticiones y 
conjuntos de datos como estimador, utilizando el algoritmo ~
\ref{alg:CMC-pvalue}. Se rechazará la hipótesis nula (la 
equivalencia de las medias) si para algún $a$ la media 
inicial $\hat{e}_a$ se encuentra en las colas de la 
distribución formada por los valores de las permutaciones $
\hat{e}_a^*$ consistente en la cdf muestral:
	\[ 
		\hat{F}_{a \cdot \cdot \cdot}(x) =
			\frac{1}{n_d n_r n_f}
			\sum\limits_{i=1}^{n_d}
				\sum\limits_{j=1}^{n_f}
					\sum\limits_{k=1}^{n_r}
						\mathbb{I}[ e_{aijk} \leq x ].
	\]
	El segundo test sí hace uso de los intervalos, notando 
por $[q_{-adf}, q_{+adf}]$ el intervalo de los errores para 
un algoritmo, base de datos y \textit{fold}. Se definen las 
cdf muestrales como
	
\begin{align*}
	\hat{F}_{-a \cdot \cdot}(x) &=
		\frac{1}{n_d n_f}
			\sum\limits_{i=1}^{n_d}
				\sum\limits_{j=1}^{n_f}
					\mathbb{I}[ q_{+aij} \leq x ], \\
	\hat{F}_{+a \cdot \cdot}(x) &=
		\frac{1}{n_d n_f}
			\sum\limits_{i=1}^{n_d}
				\sum\limits_{j=1}^{n_f}
					\mathbb{I}[ x \in [q_{-aij},q_{+aij}) ] +
			\hat{F}_{-a \cdot \cdot}(x) .			
\end{align*}

	En este test se aplica también el algoritmo 
~\ref{alg:CMC-pvalue}, rechazando la hipótesis nula si el 
intervalo $[\hat{q}_{-a}, \hat{q}_{+a}]$, con $\hat{q}_{-a}, 
\hat{q}_{+a}$ las medias de los extremos inferiores y 
superiores respectivamente para cada algoritmo con respecto a 
las conjuntos de datos y \textit{fold}, se encuentra en las colas 
de la distribución de los $[\hat{q}_{-a}^*, \hat{q}_{+a}^*]$.
\\
	Los resultados incluidos ofrecen una comparación entre la 
potencia de estos test presentados, el test de Friedman y el 
de ANOVA. Se realiza un experimento con datos sintéticos 
consistentes en los resultados de los errores de 5 algoritmos 
en 32 conjuntos de datos para una secuencia de diferencias en sus 
medias preestablecidas. Para calcular la potencia se estima 
para cada diferencia el porcentaje de experimentos para el 
que se encuentran diferencias significativas entre los 
algoritmos dos a dos. Los resultados indican una mayor 
potencia de los test presentados en este artículo.


\section{Test bayesianos}

	Se presenta en este apartado una revisión del estado del 
arte con diferentes métodos para la comparación de algoritmos
de aprendizaje automático mediante la realización de test 
estadísticos. 

\paragraph{Versión bayesiana del test de Wilcoxon} 
\cite{DBLP:conf/icml/BenavoliCMZR14} Para la comparación de algoritmos mediante
muestras apareadas, se presenta el test de Wilcoxon en 
versión bayesiana. Se realiza, al igual que en el test
de signo y el de rangos con signo, utilizando el Proceso de
Dirichlet para la distribución \textit{a priori}. La versión 
bayesiana, a diferencia de la no paramétrica, no necesita 
suponer la simetría de las muestras. Una cuestión importante
es la elección de $G_0$ en caso de que no dispongamos
información. El enfoque realizado se basa en la obtención
del límite de $DP$ para $s \rightarrow 0$. Otro enfoque 
consiste en modelar la falta de información \textit{a priori} 
como un conjunto de distribuciones de probabilidad. 

\paragraph{Procedimiento no paramétrico bayesiano para la 
comparación de algoritmos} \cite{DBLP:conf/icml/BenavoliCMZ15} En este 
artículo se presenta la versión bayesiana del test de 
Friedman. Se presenta el test, que utiliza el $DP$ para 
realizar la comparación de dos algoritmos en un primer
momento y la versión para tres o más. Una vez se ha rechazado 
la hipótesis de que los algoritmos se comporten de igual
manera, se procede realizando la comparación dos a dos 
para encontrar dónde se producen las diferencias. Se realiza 
también en este artículo un experimento comparando el test
bayesiano de Friedman con el test $F$-\textit{race}, test que
compara varios algoritmos según aquel que consiga el mejor
resultado.

\paragraph{Test estadístico para el análisis conjunto de 
medidas de rendimiento} \cite{DBLP:conf/jsai/BenavoliC15}	Para la 
comparación de algoritmos con respecto a varias medidas
se suelen utilizar o bien la media ponderada de las medidas
para cada algoritmo o bien el frente de Pareto, donde se 
consideran las soluciones no dominadas (aquellas que no son
peor con respecto a otra solución para todos los criterios).
En este artículo se presenta en primer lugar la realización 
de un test basado en la razón de verosimilitud, que presenta
las mismas desventajas que los THN. Para la realización del
test bayesiano, se opta por usar como distribución 
\textit{a priori} la distribución de Dirichlet, obteniéndose
también como distribución \textit{a posteriori} la 
distribución de Dirichlet de la que se calculan las 
probabilidades deseadas mediante muestras aleatorias.
Finalmente, se presenta una mejora de este método usando
redes bayesianas.

\paragraph{Comparación de algoritmos en múltiples bases de 
datos con CV} \cite{DBLP:journals/ml/CoraniB15} En este artículo se introduce
el $t$-test bayesiano (\ref{ssec:bayes-ttest}) y un test de 
Poisson que compara dos clasificadores en varias bases de 
datos teniendo en cuenta la correlación y la incertidumbre 
provocada por la validación cruzada en cada conjunto de 
datos.

\paragraph{Tutorial para la comparación de clasificadores 
mediante análisis bayesiano}. \cite{DBLP:journals/corr/BenavoliCDZ16} Este artículo 
es un artículo divulgativo que presenta las ventajas de los 
test bayesianos y expone su uso en diferentes casos, 
aplicando el $t$-test bayesiano, el test de signos y el $t$-
test bayesiano jerárquico (Corani, \cite{coranistatistical}), una extensión que permite realizar 
inferencia en varios conjuntos de datos.

\chapter{Software}
\label{chapter:software}

\section{Revisión software existente}

	En esta sección se incluye una descripción del software 
integrado en R.

\subsection*{JavaNPST}
	 
	Este paquete desarrollado por Derrac \textit{et. al} 
(\cite{2015arXiv150104222D}) implementado en \texttt{Java} 
contiene cuarenta test estadísticos no paramétricos de las 
siguientes familias:
\begin{itemize}
	\item Test de aleatoriedad
	\item Test de bondad del ajuste
	\item Procedimientos de una muestra y de muestras
		 apareadas
	\item Procedimientos generales de dos muestras
	\item Test para problemas de localización
	\item Test para problemas de escala
	\item Test de igualdad de muestras independientes
	\item Medidas de asociación para muestras bivariantes
	\item Medidas de asociación en clasificaciones múltiples
	\item Análisis del conteo de datos
\end{itemize}
	
	La biblioteca \texttt{JavaNPST} incluye una definición de 
las estructuras de datos (tablas y secuencias) usadas para 
el manejo y la realización de los test incluidos. En el 
artículo se incluyen un ejemplo de uso aplicado a la 
comparación de algoritmos mediante los test de 
signo y el test de Wilcoxon. 

\subsection*{scmamp} 

	Este paquete de \texttt{R} desarrollado por Calvo y 
Santafe (\cite{scmamp}) implementa test estadísticos para
la comparación de múltiples algoritmos en distintos 
problemas. Una de los principales atractivos radica en la 
disponibilidad de ejecutar los test \textit{post hoc} que
constituyen el estado del arte. Podemos encontrar los 
siguientes tipos de funciones en este paquete:

\begin{description}
	\item[Test para la comparación de múltiples algoritmos:]
		Disponemos del test paramétrico ANOVA y de test
		no paramétricos como el contraste basado en
		medianas, el test de Friedman, el de rangos alineados
		de Friedman o el test de Quade. Cada uno de ellos
		tiene un método adicional para efectuar el test
		 \textit{post hoc} asociado.
	\item[Ajuste de $p$-valores] Se incluyen los métodos
		para ajustar los $p$-valores descritos por García en
		\cite{garcia2008extension} y 
		\cite{DBLP:journals/isci/GarciaFLH10}:
		Bergmann-Hommel, Finner, Holland, Li, Rom y Shaffer.
		Se incluye una función para obtener los $p$-valores
		de las comparaciones que deseemos realizar entre
		algoritmos.
	\item[Conjuntos de datos] utilizados en distintos
		artículos sobre los que se realizan los experimentos. 
	\item[Métodos auxiliares y creación de gráficos] La
		representación de los resultados se facilita 
		con las funciones incluidas en el paquete.
\end{description}

	Junto con el paquete viene un tutorial para saber cómo
se manejan las funciones y cómo utilizar el paquete para
la comparación del rendimiento de algoritmos.

\subsection*{BayesianML-tutorial}

	Por último, presentamos los test bayesianos implementados
por Benavoli (\cite{DBLP:journals/corr/BenavoliCDZ16}) y 
disponibles en el repositorio \fnurl{\texttt{tutorial}}
{https://github.com/BayesianTestsML/tutorial/}. 
En este repositorio se encuentra la implementación en 
\texttt{R} para las versiones bayesianas de los test 
estadísticos presentados en este artículo, es decir, 
el test de Wilcoxon de rangos con signo, el test de signo
y el $t$-test correlado bayesiano. Se incluye también el
test jerárquico para la comparación sobre varios conjuntos
de datos descrito por Corani \textit{et. al} en \cite{coranistatistical}. 
El código para este método se ha realizado
en \texttt{R} y \texttt{Stan}, un lenguaje para inferencia
bayesiana. Para ilustrar la comparación, en el repositorio
se encuentran disponibles los tests en el lenguaje \texttt{Julia} 
y una serie de \textit{Notebooks} 
de este lenguaje para ilustrar su uso, acompañándolo
con gráficas de la distribución \textit{a posteriori}
de la media de las diferencias.
	
	 
\section{rNPBST}

	En esta sección se describirá la herramienta software
desarrollada. El objetivo inicial planteado consistía en
la integración en un paquete de los test estadísticos
más utilizados para la comparación de algoritmos. Para
esta labor, partimos de los paquetes descritos.

\subsection*{JavaNPST}

	La integración del paquete \texttt{JavaNPST} en
\texttt{R} se ha realizado usando \texttt{rJava}.\\
Para ello, en primer lugar se definen en \texttt{R} los 
métodos necesarios para pasar de los tipos de datos 
habituales como las matrices y los vectores a las estructuras 
usadas en el paquete  \texttt{JavaNPST}: \texttt{dataTable}, 
que son el equivalente a las matrices, y la clase abstracta 
\texttt{Sequence}  de la que heredan las clases 
\texttt{NumericSequence} y \texttt{StringSequence}, que se 
corresponden con vectores, bien de números o bien de cadenas 
de caracteres.\\
	Los test disponibles en el paquete \texttt{JavaNPST} 
se corresponden con una clase de la cual se crea un 
objeto y se le proporcionan unos datos, entonces realizamos
el test y el objeto creado nos devuelve una serie de valores.
Estos valores se corresponden con los estadísticos utilizados
en la realización del test o los $p$-valores devueltos.
La forma de llevar esto a \texttt{R} ha sido mediante
una función para cada test, donde se crea un objeto
de la clase correspondiente con los datos (bien una matriz
o una secuencia), se ejecuta el test, obteniendo el informe
generado por el paquete utilizado y finalmente se obtienen 
los estadísticos utilizados en el test y los $p$-valores
disponibles (según el test tendremos un $p$-valor o 
varios según las hipótesis alternativas). Estos dos últimos
datos, los estadísticos y los $p$-valores los introducimos
en un objeto de la clase de \texttt{R} \texttt{htest},
que frecuentemente se usa para guardar los resultados de un
test estadístico.

\subsection*{BayesianML-tutorial}

	Como se ha descrito, en este repositorio están
implementados los test bayesianos en \texttt{R}
y en \texttt{Julia}, sin embargo, la información
que devuelven es distinta: en la versión en \texttt{R}
únicamente se devuelve la probabilidad \textit{a
posteriori} de la pertenencia del parámetro 
$\theta$ la diferencia entre los clasificadores
a la región izquierda, a la \textit{rope} o a la
región derecha; mientras que en la versión en
\texttt{Julia} se devuelven una muestra de 
la distribución de Dirichlet \textit{a posteriori},
que facilita la representación gráfica.\\
Por tanto, se ha descartado la opción de 
llamar a estas funciones desde nuestro paquete
(si quisiéramos una representación gráfica
y la probabilidad de pertenencia estaríamos
realizando dos veces el test) y optamos por
escribir nuestra propia función para el
test bayesiano de signo, el test bayesiano de 
rangos con signos y la versión bayesiana 
de $t$-test correlados. Además, se ha observado 	
un tiempo de ejecución excesivo utilizando
el test bayesiano de rangos con signos (en torno
a dos minutos) en la versión del repositorio
frente a la versión implementada (unos cinco
segundos). Sin entrar en la implementación, se 
describe a continuación el algoritmo utilizado:

\begin{algorithm}[H]
	\caption{Test bayesiano de rangos con signo}
	\label{alg:TBRS}
	\begin{algorithmic}[1]
	\REQUIRE $x, y$, vectores de observaciones.
	\REQUIRE $s, z_0, weights$, parámetros \textit{a priori}.
	\REQUIRE $rope_{min}, rope_{max}$, extremos de la \textit{rope}
	\REQUIRE $n_{samples}$ Número de muestras a obtener de
		la distribución de los parámetros.
	\STATE $d \leftarrow$ Diferencia entre los vectores 
		de observaciones 
	\STATE $pesos \leftarrow$ muestra aleatoria de 
		la distribución de Dirichlet de tamaño $n_{samples}$.
	\STATE $b_l, b_{rope}, b_r \leftarrow$ vector binario con
		la pertenencia o no de cada elemento de $d$ a la 
		región izquierda, a la \textit{rope} y a la derecha.
	\FOR {$i \in [1, n_{samples}]$}
		\STATE $coef \leftarrow pesos_i^T pesos_i$
		\STATE $muestra_i \leftarrow$
			$(coef \cdot b_l, coef \cdot b_{rope}, 
			coef \cdot b_r)$,  
	\ENDFOR
	\STATE $probabilidades_{a\ posteriori} \leftarrow $
		Media del número de veces que el máximo de la fila 
		está en cada columna.
	\RETURN $muestra, probabilidades_{a\ posteriori}$
	\end{algorithmic}
\end{algorithm} 


Para la representación gráfica se ha utilizado
la representación que vemos en el artículo
de Benavoli (\cite{DBLP:journals/corr/BenavoliCDZ16}),
\texttt{R} utilizando el paquete \texttt{ggtern} (\cite{ggtern})
para la representación de ternas. Estos gráficos
muestran la nube de puntos obtenida de realizar
una muestra aleatoria de una distribución
de Dirichlet con los parámetros obtenidos.
El centro del triángulo representa el punto
$\left( \frac{1}{3}, \frac{1}{3}, \frac{1}{3} \right)$
es decir, hay una igual probabilidad para que el punto
caiga en la \textit{rope}, de un lado u otro.\\
Para el $t$-test correlado se 



\chapter{Experimentos}
\label{chapter:experimentos}
\section{Introducción al caso práctico}
	Una vez hemos presentado las principales técnicas
estadísticas que se utilizan en la comparación de algoritmos
en aprendizaje automático, realizaremos un estudio práctico
en \texttt{R} sobre la aplicación de estas técnicas.\\
	Para ello, usaremos los resultados obtenidos por cinco
algoritmos (Tabla~\ref{tab:algoritmos}) 
sobre varios conjuntos de datos (Tabla~\ref{tab:DS}). Los
conjuntos de datos constituyen un subconjunto de los 
disponibles en el repositorio 
\fnurl{KEEL}{http://www.keel.es/} 
(\cite{alcala2010keel}) para problemas de clasificación
supervisada. En dicho repositorio encontramos ya disponibles
particiones de los conjuntos de datos para facilitar que los
experimentos sean reproducibles. Se ha utilizado para cada 
conjunto de datos la partición para realizar 10-CV, esto es,
cada conjunto se particiona en 10 subconjuntos y en cada una 
de las 10 repeticiones, utilizamos un subconjunto como test y
los restantes 9 como datos de entrenamiento. Cada archivo
contiene los diez pares de ficheros con datos de
entrenamiento y test. Los algoritmos a comparar son
ampliamente conocidos y no se entra en más detalle sobre
ellos ya que la materia de este estudio no son los algoritmos
sino los test utilizados para compararlos.

\begin{table}[]
\centering
\caption{Algoritmos}
\label{tab:algoritmos}
\begin{tabularx}{\textwidth}{lX}
\toprule
Algoritmo             & Descripción                                                                                                                             \\ \midrule
\texttt{multinom}     & Regresión logística de la biblioteca \texttt{nnet} para un problema de clasificación con varias clases.                               \\
\texttt{knn}          & Algoritmo de los $k$ vecinos más cercanos de la biblioteca \texttt{class}. 
Se han utilizado los valores por defecto $k=1$, $l=0$ y la distancia euclídea.      \\
\texttt{randomForest} & Algoritmo de la biblioteca \texttt{randomForest}. Se ha fijado el parámetro \texttt{mtray} al valor por defecto $mtray = \sqrt{p}$. \\
\texttt{nnet}         & Red neuronal de la biblioteca \texttt{nnet}.                                                                                          \\
\texttt{naiveBayes}   & Clasificador \textit{naive Bayes} de la biblioteca \texttt{e1071}.                                                                  \\ \bottomrule
\end{tabularx}
\end{table}

\begin{table}[]
\centering
\caption{Conjuntos de datos}
\label{tab:DS}
\begin{tabular}{@{}lccc@{}}
\toprule
Conjunto de datos & N$\degree$ de atributos & N$\degree$ de clases & N$\degree$ de instancias \\ \midrule
abalone           & 8                   & 28               & 4174                 \\
australian        & 14                  & 2                & 690                  \\
automobile        & 25                  & 6                & 205                  \\
balance           & 4                   & 3                & 625                  \\
breast            & 9                   & 2                & 277                  \\
bupa              & 6                   & 2                & 345                  \\
car               & 6                   & 4                & 1728                 \\
cleveland         & 13                  & 5                & 303                  \\
crx               & 15                  & 2                & 690                  \\
dermatology       & 34                  & 6                & 366                  \\
german            & 20                  & 2                & 1000                 \\
glass             & 9                   & 7                & 214                  \\
hayes-roth        & 4                   & 3                & 160                  \\
heart             & 13                  & 2                & 270                  \\
ionosphere        & 33                  & 2                & 351                  \\
led7digit         & 7                   & 10               & 500                  \\
letter            & 16                  & 26               & 20000                \\
lymphography      & 18                  & 4                & 148                  \\
mushroom          & 22                  & 2                & 8124                 \\
optdigits         & 64                  & 10               & 5620                 \\
satimage          & 36                  & 7                & 6435                 \\
spambase          & 57                  & 2                & 4597                 \\
splice            & 60                  & 3                & 3190                 \\
tic-tac-toe       & 9                   & 2                & 958                  \\
vehicle           & 18                  & 4                & 846                  \\
vowel             & 13                  & 11               & 990                  \\
wine              & 13                  & 3                & 178                  \\
yeast             & 8                   & 10               & 1484                 \\
zoo               & 16                  & 7                & 101                  \\ \bottomrule
\end{tabular}
\end{table}

\section{Aplicación de test estadísticos}

	Esta sección se puede considerar como un ejemplo de qué
pasos seguiríamos al comparar varios algoritmos sobre
múltiples bases de datos.\\
	Por tener una idea de la distribución del error de los 
algoritmos a comparar, y así tener una primera estimación
de la normalidad de la población, condición necesaria para
los test paramétricos, mostramos el gráfico con la 
distribución de los errores obtenidos:

\begin{figure}[H]
\centering
\label{fig:densplot}
\caption{Gráfico de densidad}
\includegraphics[width=0.5\textwidth]{imagenes/DensityPlot.pdf}
\end{figure}

	En esta gráfica podemos observar que las distribución del
error no es simétrica ni unimodal, con lo que no podemos
suponer la normalidad de la distribución. Realizamos en 
primer lugar como ya se ha descrito a lo largo de la memoria
el test con hipótesis nula la igualdad de las medias del
rendimiento de los algoritmos con nivel de significación
$\alpha = 0.05$. Para ello podemos usar
los test no paramétricos de Friedman, el test de
rangos alineados de Friedman, el de Iman-Davenport o el
test de Quade. Al aplicar cualquiera de estos test sobre
los resultados obtenidos de los algoritmos se obtiene
un $p$-valor mucho menor que $\alpha$. En cambio, 
si realizamos el test ANOVA, no rechazamos la hipótesis
nula, lo que hace pensar que para esta pequeña cantidad de
datos y sobre todo debido a la falta de normalidad, la
potencia es inferior. Como primer método \textit{post hoc}
a utilizar, escogemos el método de Nemenyi, basado en la
diferencia de las medias de los ránquines de los 
clasificadores. Consideraremos que dos algoritmos tienen
resultados distinguibles si la diferencia en valor absoluto
es mayor que el estadístico obtenido. En nuestro caso, 
se obtiene que únicamente \texttt{random.forest} es distinto
a los otros cuatro, entre los que no se encuentran 
diferencias significativas (Tabla~\ref{tab:Nemenyi}). Para ilustrar este resultado
se incluye la representación de la diferencia crítica (Figura~\ref{fig:cdplot}).

\begin{table}[]
\centering
\caption{Test de Nemenyi}
\label{tab:Nemenyi}
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|l|l|l|l|l|}
\hline
                    & logistic.regression & KNN                & random.forest     & naive.bayes        & neural.network     \\ \hline
logistic.regression & 0                   & -0.689655172413793 & 1.41379310344828  & -0.896551724137931 & -0.344827586206896 \\ \hline
KNN                 & -0.689655172413793  & 0                  & 2.10344827586207  & -0.206896551724138 & 0.344827586206897  \\ \hline
random.forest       & 1.41379310344828    & 2.10344827586207   & 0                 & -2.31034482758621  & -1.75862068965517  \\ \hline
naive.bayes         & -0.896551724137931  & -0.206896551724138 & -2.31034482758621 & 0                  & 0.551724137931035  \\ \hline
neural.network      & -0.344827586206896  & 0.344827586206897  & -1.75862068965517 & 0.551724137931035  & 0                  \\ \hline
\end{tabular}}
\end{table}

\begin{figure}[H]
\label{fig:cdplot}
\centering
\caption{Critical Difference Plot}
\includegraphics[width=0.7\textwidth]{imagenes/CDplot.pdf}
\end{figure}

	También podemos utilizar los procedimientos \textit{post
hoc} asociados a los test, especialmente debido a que no
se da la normalidad de los datos, teniendo en cuenta que
debemos ajustar los $p$-valores con alguno de los métodos
vistos anteriormente. En nuestro caso, ningún test 
\textit{post hoc} a presentado una mayor potencia que el
test de Nemenyi, pues no se distinguen más diferencias que 
entre el algoritmo \texttt{random.forest} y los demás y, por
ejemplo no se encuentras diferencias significativas entre  
regresión logística y \textit{random forest}. En la Figura\ref{fig:grafo}
se representan mediante un grafo las diferencias encontradas
entre los algoritmos, existiendo un arco entre aquellos 
para los cuales no se descarta la hipótesis nula.

\begin{figure}[H]
\centering
\caption{Grafo}
\label{fig:grafo}
\includegraphics[width=0.5\textwidth]{imagenes/AlgGraphFinner.pdf}
\end{figure}

\subsection*{Test bayesianos}

	Podemos realizar también los test bayesianos implementados, 
que ofrecen unos resultados más intuitivos al representarlos
gráficamente.\\
	El primer test para el que mostraremos resultados es
la versión bayesiana del $t$-test correlado. Este test
se realiza sobre un par de algoritmos aplicados sobre un
problema. Los datos de entrada son las $k$ medidas tomadas
para cada partición en entrenamiento y test, y tomaremos 
como correlación por defecto $\rho = \frac{1}{k}$, pues
supondremos que estamos realizando validación cruzada y 
por tanto el número de datos usados para validación y el 
número de datos totales seguirá la razón $\frac{1}{k}$.\\
Habiendo comprobado ya con métodos frecuentistas dónde se
sitúan las principales diferencias entre algoritmos, se
muestran ejemplos con distintos resultados.

\begin{figure}[H]
\centering
\subfloat[$\theta_l=0.457$,$ \theta_{rope}=0.158$, $\theta_r=0.385$]{
   \label{fig:ttbc1}
   \includegraphics[width=0.5\textwidth]{imagenes/KNN-LR-automobile.pdf}}
\subfloat[$\theta_l=0.997, \theta_{rope}=0.001, \theta_r=0.002$]{
   \label{fig:ttbc2}
   \includegraphics[width=0.5\textwidth]{imagenes/LR-RF-hayes-roth.pdf}}
   \newline
   \centering
   \subfloat[$\theta_l=0.050, \theta_{rope}=0.93, \theta_r=0.02$]{
   \label{fig:ttbc3}
   \includegraphics[width=0.5\textwidth]{imagenes/KNN-RF-zoo.pdf}}
\caption{Resultados de $t$-test correlado bayesiano}
\end{figure}

En la comparación que mostramos en la Figura\ref{fig:ttbc1} 
observamos que no podemos deducir nada. En la Figura \ref{fig:ttbc2}, 
al estar la distribución prácticamente fuera de la
 \textit{rope} en su totalidad, deducimos que para el 
 conjunto de datos de \textit{hayes-roth} el algoritmo
 \texttt{random forest} funciona mejor que la regresión 
 logística. Por último, en la Figura \ref{fig:ttbc3} la
 mayor parte de la distribución se encuentra \textit{rope}
 (habiendo tomado como extremos -0.05, 0.05 en este caso),
 con lo que podemos deducir que el rendimiento del \texttt{KNN} y del algoritmo \texttt{random forest} es 
 equivalente en este caso.\\
 
 Pasamos entonces al problema de la comparación del
rendimiento para múltiples problemas. Aquí, no usamos los
datos sobre cada conjunto de datos sino los resultados 
promediados entre las distintas particiones. Para realizar
esta comparación usamos tanto el test bayesiano de rangos
con signos como el test de signo. Al igual que para los 
test anteriores, mostramos sus resultados de manera gráfica.
Para la obtención de los datos, se obtiene una muestra de la distribución \textit{a posteriori}:

\begin{figure}[H]
\centering
\subfloat[Test bayesiano de rangos alineados]{
   \label{fig:tbra1}
   \includegraphics[width=0.5\textwidth]{imagenes/BSR-RF-NB.pdf}}
\caption{Comparación entre \textit{random forest} y \textit{naive bayes}}
\end{figure}

	En esta figura se aprecia que la distribución a posteriori se sitúan a la derecha de la \textit{rope}, lo que significa que, según los datos obtenidos, \textit{random forest} tiene un mejor desempeño que el clasificador \textit{naive bayes} con un 100$\%$ de probabilidad. La probabilidad de que esto ocurra usando el test bayesiano de signo baja al $86\%$.

\begin{figure}[H]
\centering
\subfloat[Test bayesiano de signo]{
   \label{fig:tbs2}
   \includegraphics[width=0.5\textwidth]{imagenes/BS-KNN-NNET.pdf}}
\subfloat[Test bayesiano de rangos alineados]{
   \label{fig:tbra2}
   \includegraphics[width=0.5\textwidth]{imagenes/BSR-KNN-NNET.pdf}}
\caption{Comparación entre \texttt{KNN} y red neuronal, tomando como extremos de la \textit{rope} -0.02, 0.02}
\end{figure}

	Por otra parte, considerando que dos algoritmos tienen un 
rendimiento equivalente si su diferencia es menor a un $2\%$, 
observamos que el test de signo da una probabilidad casi equivalente para cada región, y que en cambio el test de rangos alineados da más del $50\%$ de probabilidad a que los algoritmos sean equivalentes.



