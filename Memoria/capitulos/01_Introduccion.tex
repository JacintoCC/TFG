\chapter*{Planteamiento}\addcontentsline{toc}{chapter}{Planteamiento}
\markboth{Planteamiento}{}
\pagenumbering{arabic}
\setcounter{page}{1}

\section*{Contextualización}\addcontentsline{toc}{section}{Contextualización}
	
	El interés creciente en el campo del aprendizaje
automático ha significado un aumento en la propuesta
de nuevos algoritmos que pretenden mejorar a los 
ya existentes. Se describe a continuación el proceso
general para poder comparar algoritmos, basándonos en
las recomendaciones realizadas por Alpaydin 
(\cite{Alpaydin:2010:IML:1734076}) y Japkowicz  
(\cite{DBLP:books/cu/Japkowicz2011}).\\
	A la hora de evaluar un algoritmo,
nos interesa conocer el error asociado a un problema y 
su relación con respecto a otros. Sin embargo, debemos ser 
conscientes de que para ello debemos diseñar y realizar un 
experimento cuyos resultados sean fiables y minimizar el 
efecto producido por las circunstancias en las que se 
realizan o los datos disponibles. Por ejemplo, el error 
obtenido en los datos utilizados para entrenar el modelo 
(error de entrenamiento) será menor que el error en unos 
datos que no han participado en este entrenamiento y son 
usados para evaluar el modelo (error de test). Cabe mencionar 
también que el error de entrenamiento no es comparable debido 
a que modelos más complejos se ajustarán más a los datos de 
entrenamiento, sin que eso signifique que el modelo se 
comporte mejor de manera global. Hay algoritmos que se 
comportan de manera no determinista, y por tanto deseemos 
realizar varias ejecuciones para estimar circunstancias 
aleatorias. En definitiva, debemos basar nuestra evaluación 
del algoritmo en la distribución (desconocida) del error. 
Para una correcta estimación del error,  es importante tener 
en cuenta lo siguiente:
	
	\begin{itemize}
	\item El error depende del problema. El teorema de 
		\textit{No Free Lunch} (Wolpert y Macready, 
		\cite{Wolpert:1997:NFL:2221336.2221408}) 
		afirma que dados dos 
		algoritmos su rendimiento medio para todos los 
		posibles problemas es el mismo, lo que se traduce en 
		que según el problema se puede dar que un algoritmo 
		sea mejor que otro o al contrario.
	\item La partición de la base de datos referente al 
		problema en datos de entrenamiento y de test se 
		realiza para obtener una idea más acertada del error 
		cometido por el algoritmo para el problema. Sin 
		embargo, una vez que se considere válido el algoritmo 
		se entrenará con todos los datos disponibles.
	\item Para validar el algoritmo, se deben utilizar datos 
		que no hayan sido utilizados anteriormente de ninguna 
		manera.
	\item Normalmente, para la comparación del rendimiento de 
		algoritmos se usa el error cometido en la 
		clasificación, aunque se podrían considerar otros 
		factores como el tiempo necesario para entrenar el 
		algoritmo, su simplicidad...
	\end{itemize}
	
\subsection*{Diseño de experimentos}

	Al hablar de experimentos nos referimos al test o a la 
serie de test donde se manejan factores que afectan a la 
salida. Pretendemos minimizar aquellos factores que no se 
controlan y obtener resultados estadísticamente 
significativos. Para ajustar parámetros propios del modelo de 
aprendizaje automático que estemos utilizando existen 
diferentes aproximaciones: la \textit{mejor suposición} 
(donde quien realiza el experimento ajusta los parámetros 
basándose en su conocimiento sobre el problema), \textit{un 
factor cada vez} (se supone que los factores son 
independientes y se prueban diferentes valores para cada 
parámetro de uno en uno, mientras los demás se mantienen en 
una posición base) y \textit{diseño factorial} (se crea una 
rejilla con diferentes posibles valores para cada parámetro y 
se prueba en ellos, es más costoso).\\

	Para minimizar el número de ejecuciones necesarias, se 
suele utilizar conocimiento previo para reducir las 
combinaciones a probar. Otra estrategia consiste en el diseño 
de la superficie de respuesta. Se considera la siguiente 
situación:
		\[ r = g( f_1, \dots, f_F | \phi ), \]
	donde $r$ es la respuesta, $g$ el modelo utilizado, 
$f_1, \dots f_F$ los factores y $\phi$ la estimación empírica 
del modelo para cada configuración particular probada. El 
procedimiento consiste en ir añadiendo a $\phi$ las 
evaluaciones realizadas, ajustar $g$ según $\phi$ y buscar su 
máximo (o mínimo), evaluar el algoritmo en ese punto y 
ajustar $\phi$.\\
	En la realización de experimentos es necesaria la 
aleatorización a la hora de probar las diferentes 
configuraciones de parámetros (no tanto en nuestro contexto, 
pero el orden de las configuraciones al realizar experimentos 
donde interviene maquinaria por ejemplo podría interferir en los 
resultados debido a factores como la temperatura de la 
máquina). Otro factor a tener en cuenta es que es necesaria 
la repetición de las ejecuciones y promediar los resultados 
para así reducir el impacto de factores aleatorios. Para 
reducir la variabilidad de factores que no dependen del 
propio algoritmo a evaluar, como las bases de datos o las 
particiones realizadas, en la comparación de varios 
algoritmos utilizamos las mismas particiones para cada uno.

\paragraph{Directrices a la hora de realizar un experimento}
	\begin{enumerate}
	\item Fijar la intención del estudio
	\item Seleccionar las variables de respuesta
	\item Seleccionar los factores con los que se realizará 
		el experimento y los niveles a comprobar
	\item Diseño del experimento: La partición para 
		entrenamiento y test depende del tamaño de la base de 
		datos. Si la base de datos es pequeña puede haber una 
		alta variabilidad y obtener resultados no 
		concluyentes.
	\item Efectuar el experimento: Antes de llevar a cabo un 
		experimento de gran magnitud es aconsejable probar 
		pequeños conjuntos de datos para probar que funciona. 
		También es interesante guardar resultados intermedios 
		o las semillas aleatorias para facilitar la 
		reproducción de los datos.
	\item El experimentador debe ser imparcial y juzgar de 
		igual manera un algoritmo que otro, realizar 
		documentación ...
	\item Análisis estadístico: las afirmaciones y preguntas 
		que se realicen debe sostenerse estadísticamente 
		hablando. Es aconsejable un análisis visual para 
		exponer los datos obtenidos.
	\end{enumerate}
	
\subsection*{Test de hipótesis}

	Una vez se han extraído los datos, pretendemos obtener
las conclusiones de manera estadística. Para ello, la 
forma habitual de proceder es mediante los test de hipótesis.
En la realización de los test de hipótesis, el 
procedimiento es el siguiente: se define un estadístico que 
sigue una distribución conocida en el caso de que se cumpla 
la hipótesis realizada. Entonces si el estadístico calculado 
de la muestra tiene poca probabilidad de obtenerse de la 
distribución, se rechaza la hipótesis; en caso contrario, no. 
\\
	Surgen entonces las cuestiones: ¿Se pueden justificar los 
resultados estadísticamente o se obtienen por azar? ¿Son los 
conjuntos de datos representativos para el problema? Estas 
preguntas no se pueden responder de forma exhaustiva, es la 
labor de los test estadísticos reunir los datos disponibles 
para justificar la consistencia de las conclusiones. \\
	A la hora de realizar los test estadísticos podemos 
realizar las siguientes comparaciones:
	\begin{enumerate}
	\item Comparación de dos algoritmos en un dominio: $t$-
		test (paramétrico), test de McNemar (no paramétrico).
	\item Comparación de dos algoritmos en varios dominios: 
		\textit{Signed-Rank test} de Wilcoxon y test de signo 
		(no paramétricos).
	\item Comparación de múltiples algoritmos en varios 
		dominios: ANOVA (paramétrico), test de Friedman o 
		basados en permutaciones (no paramétricos). 
	\end{enumerate}

	Nótese que el test de \textit{Signed-Rank} de Wilcoxon 
para muestras apareadas puede aplicarse tanto para la 
comparación de dos como entre varios algoritmos en un 
dominio. Los test para varios algoritmos en varias bases de 
datos necesitan un test posterior en caso de rechazar la 
hipótesis de que todos los algoritmos se comporten de igual 
manera. Ejemplos de estos test son el de Tukey, de 
Bonferroni-Dunn o de Nemenyi.

\section*{Descripción del problema abordado}\addcontentsline{toc}{section}{Descripción del problema}

	El problema abordado consiste en el estudio de las 
técnicas estadísticas más relevantes en la comparación
del rendimiento de algoritmos de aprendizaje automático. 
En el estudio teórico se hace un recorrido por los test 
de hipótesis nula. En primer lugar, se describen y 
se incluyen los test paramétricos, ampliamente conocidos 
y utilizados en este contexto. Como alternativa a los 
test paramétricos se presentan los test no paramétricos, 
indicando las diferencias con los primeros, sus ventajas
e inconvenientes, las situaciones en las que son más 
favorables y un conjunto de test no paramétricos junto con su 
descripción y forma de usarlos. Una familia concreta de 
este tipo de test son los test basados en permutaciones,
de los cuales se presenta la base matemática asociada. Por
último, y como alternativa a los test de hipótesis nula, 
se presentan los test bayesianos, que utilizan la estadística 
bayesiana para la realización de test estadísticos para
la comparación de algoritmos. Al ser las técnicas
más novedosas, se presenta la revisión de varios
artículos recientes que aplican test no paramétricos y test
bayesianos al problema en cuestión.\\
	La vertiente práctica del problema consiste en la 
aplicación de estos test sobre los resultados obtenidos
de la evaluación de unos algoritmos en una serie de
conjuntos de datos. Este estudio práctico se ha llevado
a cabo en \texttt{R}, implementando un \textit{wrapper}
para el paquete de Java \texttt{JavaNPST}, que permite
la realización de test no paramétricos.

\section*{Técnicas utilizadas}\addcontentsline{toc}{section}{Técnicas utilizadas}

	Las principal área matemática presente en este trabajo es
la inferencia estadística, de la que se utilizan conceptos
y resultados referentes a los estadísticos y sus propiedades,
los test de hipótesis nula, tanto paramétricos como no 
paramétricos y la estadística bayesiana.\\
	Los conceptos informáticos se han extraído del área
del aprendizaje automático, y se corresponden con el 
problema de clasificación, las medidas para evaluar el 
rendimiento o los algoritmos de clasificación.

\section*{Contenido de la memoria}\addcontentsline{toc}{section}{Contenido de la memoria}
	En la primera parte de la memoria
(\ref{part:matematicas}) se presenta en el 
capítulo~\ref{chapter:Intro} una introducción a la evaluación 
de algoritmos de aprendizaje automático, a las medidas 
utilizadas para ello y la notación usada. En los 
sucesivos capítulos se presenta el contenido matemático
relativo a los test paramétricos 
(Cap.~\ref{chapter:test_parametricos}), los test no
paramétricos y basados en permutaciones 
(Cap.~\ref{chapter:test_no_parametricos}) y
los test bayesianos (Cap.~\ref{chapter:test_bayesianos}).\\
	En la segunda parte de la memoria 
(\ref{part:informatica}) se hace en el capítulo~
\ref{chapter:revision} una revisión del estado del arte
en la aplicación de test estadísticos al problema que 
nos ocupa. En el Capítulo~\ref{chapter:software} se
realiza una descripción de varios paquetes software 
existentes para la realización de test estadísticos, los
cuales se integran en un paquete de \texttt{R} que funciona
como \textit{wrapper}. Finalmente,
en el capítulo~\ref{chapter:experimentos} se realiza un
caso práctico donde se aplican los test estadísticos
disponibles y se compara su potencia.

\section*{Principales fuentes utilizadas}\addcontentsline{toc}{section}{Fuentes utilizadas}

	Las principales fuentes utilizadas han sido los libros de
Sheskin
(\cite{sheskin2003handbook}), Japkowicz
(\cite{DBLP:books/cu/Japkowicz2011})
y Gibbons y Chakraborti (\cite{DBLP:reference/stat/GibbonsC11}) 
para el desarrollo matemático
y la descripción de los test estadísticos paramétricos y
no paramétricos. Los métodos paramétricos están basados 
también en los trabajos de Alpaydin (\cite{Alpaydin98combined5},
\cite{Alpaydin:2010:IML:1734076}) y Ditterich
 (\cite{dietterich1998approximate}).
Las metodologías no paramétricas se
han complementado principalmente con los artículos de 
Dem\v{s}ar (\cite{DBLP:journals/jmlr/Demsar06}) y 
García (\cite{garcia2008extension}, 
\cite{DBLP:journals/isci/GarciaFLH10}). Para los test basados
en permutaciones se ha utilizado fundamentalmente el 
libro de Pesarin y Salmaso (\cite{pesarin2010permutation}).
Los test bayesianos han sido desarrollados usando 
principalmente el tutorial de Benavoli \textit{et. al} 
(\cite{DBLP:journals/corr/BenavoliCDZ16}).\\
	Para la parte de informática, se han utilizado la
documentación disponible de los paquetes integrados
(\cite{2015arXiv150104222D}, \cite{scmamp}, 
\cite{DBLP:conf/icml/BenavoliCMZ15}).

\chapter*{Objetivos}\addcontentsline{toc}{chapter}{Objetivos}

	El primer objetivo planteado fue realizar la 
recopilación, el estudio bibliográfico y recoger la 
taxonomía de las técnicas estadísticas que se aplican
en estudios experimentales en el área del aprendizaje
automático. La realización de este objetivo se realiza
en la primera parte de la memoria (\ref{part:informatica}) y
en la sección dedicada al estudio del estado del arte 
(\ref{chapter:revision}), incluyendo también entre los
test descritos algunos relacionados con comparación a
pesar de no ser estrictamente ésta su finalidad
como los test de aleatoriedad o los de bondad del 
ajuste.\\
	El segundo fue el diseño, desarrollo
e implementación de una biblioteca que unificase los test
estadísticos más potentes y utilizados, objetivo 
realizado basándonos en bibliotecas ya creadas para 
test específicos y que han sido unificadas en una biblioteca
de \texttt{R} (\ref{chapter:software}).\\
	El último objetivo planteado fue la evaluación y el 
análisis experimental de las diferentes técnicas estadísticas 
desarrolladas, para lo que se han obtenido datos 
experimentales del rendimiento de varios algoritmos en 
múltiples conjuntos de  datos sobre los que se aplican los 
test descritos e implementados para realizar una comparación 
acerca de su potencia y la información extraída de ellos
(\ref{chapter:experimentos}).



	

